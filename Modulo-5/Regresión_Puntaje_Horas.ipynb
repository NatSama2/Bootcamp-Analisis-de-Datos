{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NatSama2/Bootcamp-Analisis-de-Datos/blob/main/Modulo-5/Regresi%C3%B3n_Puntaje_Horas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regresi贸n Lineal Simple MCO"
      ],
      "metadata": {
        "id": "tBLHSoYlqQZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import probplot, shapiro, jarque_bera, normaltest, anderson, kstest\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan, het_white,lilliefors\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.regression.linear_model import GLS, GLSAR\n"
      ],
      "metadata": {
        "id": "QLwIOX46G7Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(\"/content/sample_data/Data_Regresion.xlsx\", sheet_name='data')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "YCcNF3LEmiBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTWmZ76Oj1rF"
      },
      "outputs": [],
      "source": [
        "# Este Script sera modificado para tu conveniencia\n",
        "\n",
        "#1 Definir variables dependiente (Y) e independiente (X)\n",
        "\n",
        "X = data['Horas(x)']\n",
        "y= data['Puntaje(y)']\n",
        "\n",
        "# A帽adir una constante a las caracter铆sticas para el modelo statsmodels\n",
        "X_const = sm.add_constant(X)\n",
        "\n",
        "print(\"Datos Cargados:\")\n",
        "print(data)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2. Construcci贸n del Modelo de Regresi贸n Lineal ---\n",
        "# Usamos statsmodels para tener un resumen detallado y acceso a los residuos para los supuestos.\n",
        "model = sm.OLS(y, X_const)\n",
        "## results = model.fit(cov_type='HAC', cov_kwds={'maxlags':1})\n",
        "results = model.fit()\n",
        "print(\"Resumen del Modelo de Regresi贸n Lineal:\")\n",
        "print(results.summary())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Obtener los residuos del modelo\n",
        "residuals = results.resid\n",
        "fitted_values = results.fittedvalues\n",
        "\n",
        "# --- 3. Evaluaci贸n de Supuestos de la Regresi贸n Lineal ---\n",
        "\n",
        "## Supuesto 1: Linealidad \n",
        "# Se asume que la relaci贸n entre las variables es lineal.\n",
        "# Se puede verificar con un gr谩fico de dispersi贸n de la variable predictora contra la variable objetivo,\n",
        "# y tambi茅n con un gr谩fico de residuos vs. valores predichos.\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Gr谩fico de dispersi贸n de X vs. Y\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.scatterplot(x=X, y=y)\n",
        "plt.title('Linealidad: X vs. Y')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "\n",
        "# Residuos vs. Valores Predichos: La mejor manera de verificar la linealidad y homocedasticidad\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.scatterplot(x=fitted_values, y=residuals)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuos vs. Valores Predichos')\n",
        "plt.xlabel('Valores Predichos')\n",
        "plt.ylabel('Residuos')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Evaluaci贸n del Supuesto de Linealidad ---\")\n",
        "print(\"Observar el gr谩fico 'X vs. Y': si los puntos siguen una tendencia lineal, el supuesto se cumple.\")\n",
        "print(\"En el gr谩fico de 'Residuos vs. Valores Predichos', los residuos deben estar distribuidos aleatoriamente alrededor de cero, sin patrones evidentes (forma de embudo, curva, etc.).\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "## Supuesto 2: Normalidad de los Residuos \n",
        "# Los residuos deben seguir una distribuci贸n normal.\n",
        "# Se puede verificar con gr谩ficos (histograma, Q-Q plot) y pruebas estad铆sticas.\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Histograma de los residuos\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.title('Histograma de los Residuos')\n",
        "plt.xlabel('Residuos')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "# Q-Q plot de los residuos\n",
        "plt.subplot(1, 2, 2)\n",
        "probplot(residuals, dist=\"norm\", plot=plt)\n",
        "plt.title('Q-Q Plot de los Residuos')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Evaluaci贸n del Supuesto de Normalidad de los Residuos ---\")\n",
        "print(\"Pruebas estad铆sticas:\")\n",
        "\n",
        "# Prueba de Shapiro-Wilk (para n < 50)\n",
        "shapiro_test = shapiro(residuals)\n",
        "print(f\"Shapiro-Wilk Test: Estad铆stico={shapiro_test.statistic:.3f}, p-value={shapiro_test.pvalue:.3f}\")\n",
        "if shapiro_test.pvalue > 0.05:\n",
        "    print(\"  -> **No se puede rechazar la hip贸tesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"  -> **Se rechaza la hip贸tesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "# Prueba de Jarque-Bera (para n > 2000, pero tambi茅n 煤til para n peque帽as)\n",
        "jarque_bera_test = jarque_bera(residuals)\n",
        "print(f\"Jarque-Bera Test: Estad铆stico={jarque_bera_test.statistic:.3f}, p-value={jarque_bera_test.pvalue:.3f}\")\n",
        "if jarque_bera_test.pvalue > 0.05:\n",
        "    print(\"  -> **No se puede rechazar la hip贸tesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"  -> **Se rechaza la hip贸tesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "\n",
        "\n",
        "# Prueba de Agostino-Pearson (D'Agostino's K-squared test)\n",
        "agostino_test = normaltest(residuals)\n",
        "print(f\"\\nAgostino-Pearson Test: Estad铆stico={agostino_test.statistic:.3f}, p-value={agostino_test.pvalue:.3f}\")\n",
        "if agostino_test.pvalue > 0.05:\n",
        "    print(\"  -> **No se puede rechazar la hip贸tesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"  -> **Se rechaza la hip贸tesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "# Prueba de Anderson-Darling\n",
        "anderson_test = anderson(residuals, dist='norm')\n",
        "print(f\"\\nAnderson-Darling Test: Estad铆stico={anderson_test.statistic:.3f}\")\n",
        "print(\"Valores cr铆ticos y niveles de significancia:\")\n",
        "for i in range(len(anderson_test.critical_values)):\n",
        "    sl, cv = anderson_test.significance_level[i], anderson_test.critical_values[i]\n",
        "    if anderson_test.statistic < cv:\n",
        "        print(f\"  -> Para nivel de significancia {sl}%: estad铆stico < {cv:.3f} (No rechazar normalidad)\")\n",
        "    else:\n",
        "        print(f\"  -> Para nivel de significancia {sl}%: estad铆stico >= {cv:.3f} (Rechazar normalidad)\")\n",
        "\n",
        "# Prueba de Kolmogorov-Smirnov (con par谩metros estimados)\n",
        "ks_test = kstest(residuals, 'norm', args=(residuals.mean(), residuals.std()))\n",
        "print(f\"\\nKolmogorov-Smirnov Test: Estad铆stico={ks_test.statistic:.3f}, p-value={ks_test.pvalue:.3f}\")\n",
        "if ks_test.pvalue > 0.05:\n",
        "    print(\"  -> **No se puede rechazar la hip贸tesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"  -> **Se rechaza la hip贸tesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "# Prueba de Lilliefors (Kolmogorov-Smirnov modificada para normalidad con par谩metros estimados)\n",
        "lillie_test = lilliefors(residuals, dist='norm')\n",
        "print(f\"\\nLilliefors Test: Estad铆stico={lillie_test[0]:.3f}, p-value={lillie_test[1]:.3f}\")\n",
        "if lillie_test[1] > 0.05:\n",
        "    print(\"  -> **No se puede rechazar la hip贸tesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"  -> **Se rechaza la hip贸tesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nObservar el Q-Q Plot: los puntos deben seguir aproximadamente la l铆nea recta. Las desviaciones indican no normalidad.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "## Supuesto 3: Homocedasticidad (Varianza Constante de los Residuos) \n",
        "# La varianza de los residuos debe ser constante en todos los niveles de las variables predictoras.\n",
        "# Se verifica principalmente con el gr谩fico de residuos vs. valores predichos y pruebas estad铆sticas.\n",
        "\n",
        "print(\"\\n--- Evaluaci贸n del Supuesto de Homocedasticidad ---\")\n",
        "# El gr谩fico de residuos vs. valores predichos ya se mostr贸 en la secci贸n de linealidad.\n",
        "# Buscar patrones en forma de embudo o cono, que indicar铆an heterocedasticidad.\n",
        "\n",
        "print(\"Pruebas estad铆sticas:\")\n",
        "\n",
        "# Prueba de Breusch-Pagan\n",
        "bp_test = het_breuschpagan(residuals, X_const)\n",
        "print(f\"Breusch-Pagan Test: LM Statistic={bp_test[0]:.3f}, p-value={bp_test[1]:.3f}, F-statistic={bp_test[2]:.3f}, F-p-value={bp_test[3]:.3f}\")\n",
        "if bp_test[1] > 0.05:\n",
        "    print(\"  -> **No se puede rechazar la hip贸tesis nula**: hay homocedasticidad (p > 0.05).\")\n",
        "else:\n",
        "    print(\"  -> **Se rechaza la hip贸tesis nula**: hay heterocedasticidad (p <= 0.05).\")\n",
        "\n",
        "# Prueba de White\n",
        "white_test = het_white(residuals, X_const)\n",
        "print(f\"White Test: LM Statistic={white_test[0]:.3f}, p-value={white_test[1]:.3f}, F-statistic={white_test[2]:.3f}, F-p-value={white_test[3]:.3f}\")\n",
        "if white_test[1] > 0.05:\n",
        "    print(\"  -> **No se puede rechazar la hip贸tesis nula**: hay homocedasticidad (p > 0.05).\")\n",
        "else:\n",
        "    print(\"  -> **Se rechaza la hip贸tesis nula**: hay heterocedasticidad (p <= 0.05).\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "## Supuesto 4: Ausencia de Multicolinealidad \n",
        "# Las variables predictoras no deben estar altamente correlacionadas entre s铆.\n",
        "# Para este conjunto de datos, solo hay una variable predictora (X), por lo que la multicolinealidad no es un problema.\n",
        "# Sin embargo, se incluye la secci贸n para demostrar c贸mo se har铆a con m煤ltiples variables.\n",
        "\n",
        "print(\"\\n--- Evaluaci贸n del Supuesto de Ausencia de Multicolinealidad ---\")\n",
        "if X.ndim > 1 and X.shape[1] > 1: # Solo se aplica si hay m谩s de una variable predictora\n",
        "    print(\"Matriz de Correlaci贸n entre Variables Predictoras:\")\n",
        "    correlation_matrix = X.corr()\n",
        "    print(correlation_matrix)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title('Matriz de Correlaci贸n de Variables Predictoras')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nFactor de Inflaci贸n de la Varianza (VIF):\")\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"feature\"] = X.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "    print(vif_data)\n",
        "\n",
        "    print(\"\\nInterpretaci贸n del VIF:\")\n",
        "    print(\"  - VIF = 1: No hay correlaci贸n.\")\n",
        "    print(\"  - VIF entre 1 y 5: Moderada correlaci贸n.\")\n",
        "    print(\"  - VIF > 5 (o > 10, seg煤n la literatura): Alta multicolinealidad, lo que puede ser problem谩tico.\")\n",
        "else:\n",
        "    print(\"Solo hay una variable predictora (X), por lo que la multicolinealidad no es un problema en este modelo.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "## Supuesto 5: Independencia de los Residuos (Ausencia de Autocorrelaci贸n) \n",
        "# Los residuos no deben estar correlacionados entre s铆. Esto es crucial en series de tiempo.\n",
        "# Se verifica con el estad铆stico de Durbin-Watson, que est谩 incluido en el resumen de `statsmodels`.\n",
        "\n",
        "print(\"\\n--- Evaluaci贸n del Supuesto de Independencia de los Residuos ---\")\n",
        "# Extrae la estadistica de Durbiny-Watson\n",
        "summary_string = str(results.summary())\n",
        "durbin_watson_line = [line for line in summary_string.split('\\n') if 'Durbin-Watson' in line][0]\n",
        "durbin_watson_stat = float(durbin_watson_line.split()[-1])\n",
        "\n",
        "print(f\"Estad铆stico Durbin-Watson (del resumen del modelo): {durbin_watson_stat:.3f}\")\n",
        "print(\"Interpretaci贸n del Durbin-Watson:\")\n",
        "print(\"  - Valor **cercano a 2**: No hay autocorrelaci贸n.\")\n",
        "print(\"  - Valor **< 2**: Autocorrelaci贸n positiva (residuos adyacentes similares).\")\n",
        "print(\"  - Valor **> 2**: Autocorrelaci贸n negativa (residuos adyacentes opuestos).\")\n",
        "print(\"  - Un valor entre 1.5 y 2.5 generalmente se considera aceptable.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"\\n--- Conclusi贸n de la Evaluaci贸n de Supuestos ---\")\n",
        "print(\"La evaluaci贸n de estos supuestos es crucial para confiar en las inferencias del modelo de regresi贸n lineal.\")\n",
        "print(\"Si los supuestos no se cumplen, las estimaciones de los coeficientes y sus errores est谩ndar pueden no ser v谩lidas,\")\n",
        "print(\"afectando la significancia estad铆stica y la capacidad de generalizaci贸n del modelo.\")\n",
        "print(\"En caso de incumplimiento, se pueden considerar transformaciones de datos, la inclusi贸n de otras variables,\")\n",
        "print(\"o el uso de modelos de regresi贸n m谩s avanzados (ej. regresi贸n robusta, modelos generalizados lineales).\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(\"/content/sample_data/Data_Regresion.xlsx\", sheet_name='data')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "xbFlGexBgC_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##M铆nimos Cuadrados Generalizados Factibles con GLSAR"
      ],
      "metadata": {
        "id": "GsH3I837kzkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import probplot, shapiro, jarque_bera, normaltest, anderson, kstest\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan, het_white,lilliefors\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.regression.linear_model import GLS, GLSAR\n",
        "\n",
        "\n",
        "\n",
        "# Transformar 'Puntaje(y)' (as per your original code)\n",
        "#data['Puntaje(y)'] = data['Puntaje(y)']**3\n",
        "\n",
        "# --- 1. Definir variables dependiente (Y) e independiente (X) ---\n",
        "X = data[['Horas(x)']] # X debe ser un DataFrame para sm.add_constant\n",
        "y = data['Puntaje(y)']\n",
        "\n",
        "# A帽adir una constante a las caracter铆sticas para el modelo statsmodels\n",
        "X_const = sm.add_constant(X)\n",
        "\n",
        "print(\"Datos Cargados:\")\n",
        "print(data)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- 2. Construcci贸n del Modelo de Regresi贸n Lineal (OLS inicial para diagn贸stico) ---\n",
        "# Usamos statsmodels para tener un resumen detallado y acceso a los residuos para los supuestos.\n",
        "\n",
        "model_ols = sm.OLS(y, X_const)\n",
        "results_ols = model_ols.fit()\n",
        "print(\"Resumen del Modelo de Regresi贸n Lineal (OLS Inicial):\")\n",
        "print(results_ols.summary())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Obtener los residuos del modelo OLS\n",
        "residuals_ols = results_ols.resid\n",
        "fitted_values_ols = results_ols.fittedvalues\n",
        "\n",
        "# --- 3. Evaluaci贸n de Supuestos de la Regresi贸n Lineal (con OLS) ---\n",
        "\n",
        "## Supuesto 1: Linealidad \n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.scatterplot(x=X['Horas(x)'], y=y)\n",
        "plt.title('Linealidad: X vs. Y (OLS)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.scatterplot(x=fitted_values_ols, y=residuals_ols)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuos vs. Valores Predichos (OLS)')\n",
        "plt.xlabel('Valores Predichos')\n",
        "plt.ylabel('Residuos')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Evaluaci贸n del Supuesto de Linealidad (OLS) ---\")\n",
        "print(\"Observar el gr谩fico 'X vs. Y': si los puntos siguen una tendencia lineal, el supuesto se cumple.\")\n",
        "print(\"En el gr谩fico de 'Residuos vs. Valores Predichos', los residuos deben estar distribuidos aleatoriamente alrededor de cero, sin patrones evidentes (forma de embudo, curva, etc.).\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "## Supuesto 2: Normalidad de los Residuos \n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(residuals_ols, kde=True)\n",
        "plt.title('Histograma de los Residuos (OLS)')\n",
        "plt.xlabel('Residuos')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "probplot(residuals_ols, dist=\"norm\", plot=plt)\n",
        "plt.title('Q-Q Plot de los Residuos (OLS)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Evaluaci贸n del Supuesto de Normalidad de los Residuos (OLS) ---\")\n",
        "print(\"Pruebas estad铆sticas:\")\n",
        "\n",
        "shapiro_test = shapiro(residuals_ols)\n",
        "print(f\"Shapiro-Wilk Test: Estad铆stico={shapiro_test.statistic:.3f}, p-value={shapiro_test.pvalue:.3f}\")\n",
        "if shapiro_test.pvalue > 0.05:\n",
        "    print(\"   -> **No se puede rechazar la hip贸tesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"   -> **Se rechaza la hip贸tesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "jarque_bera_test = jarque_bera(residuals_ols)\n",
        "print(f\"Jarque-Bera Test: Estad铆stico={jarque_bera_test.statistic:.3f}, p-value={jarque_bera_test.pvalue:.3f}\")\n",
        "if jarque_bera_test.pvalue > 0.05:\n",
        "    print(\"   -> **No se puede rechazar la hip贸tesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"   -> **Se rechaza la hip贸tesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "agostino_test = normaltest(residuals_ols)\n",
        "print(f\"\\nAgostino-Pearson Test: Estad铆stico={agostino_test.statistic:.3f}, p-value={agostino_test.pvalue:.3f}\")\n",
        "if agostino_test.pvalue > 0.05:\n",
        "    print(\"   -> **No se puede rechazar la hip贸tesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"   -> **Se rechaza la hip贸tesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "anderson_test = anderson(residuals_ols, dist='norm')\n",
        "print(f\"\\nAnderson-Darling Test: Estad铆stico={anderson_test.statistic:.3f}\")\n",
        "print(\"Valores cr铆ticos y niveles de significancia:\")\n",
        "for i in range(len(anderson_test.critical_values)):\n",
        "    sl, cv = anderson_test.significance_level[i], anderson_test.critical_values[i]\n",
        "    if anderson_test.statistic < cv:\n",
        "        print(f\"   -> Para nivel de significancia {sl}%: estad铆stico < {cv:.3f} (No rechazar normalidad)\")\n",
        "    else:\n",
        "        print(f\"   -> Para nivel de significancia {sl}%: estad铆stico >= {cv:.3f} (Rechazar normalidad)\")\n",
        "\n",
        "ks_test = kstest(residuals_ols, 'norm', args=(residuals_ols.mean(), residuals_ols.std()))\n",
        "print(f\"\\nKolmogorov-Smirnov Test: Estad铆stico={ks_test.statistic:.3f}, p-value={ks_test.pvalue:.3f}\")\n",
        "if ks_test.pvalue > 0.05:\n",
        "    print(\"   -> **No se puede rechazar la hip贸tesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"   -> **Se rechaza la hip贸tesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "lillie_test = lilliefors(residuals_ols, dist='norm')\n",
        "print(f\"\\nLilliefors Test: Estad铆stico={lillie_test[0]:.3f}, p-value={lillie_test[1]:.3f}\")\n",
        "if lillie_test[1] > 0.05:\n",
        "    print(\"   -> **No se puede rechazar la hip贸tesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"   -> **Se rechaza la hip贸tesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "print(\"\\nObservar el Q-Q Plot: los puntos deben seguir aproximadamente la l铆nea recta. Las desviaciones indican no normalidad.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "## Supuesto 3: Homocedasticidad (Varianza Constante de los Residuos) \n",
        "print(\"\\n--- Evaluaci贸n del Supuesto de Homocedasticidad (OLS) ---\")\n",
        "print(\"Pruebas estad铆sticas:\")\n",
        "\n",
        "bp_test = het_breuschpagan(residuals_ols, X_const)\n",
        "print(f\"Breusch-Pagan Test: LM Statistic={bp_test[0]:.3f}, p-value={bp_test[1]:.3f}, F-statistic={bp_test[2]:.3f}, F-p-value={bp_test[3]:.3f}\")\n",
        "if bp_test[1] > 0.05:\n",
        "    print(\"   -> **No se puede rechazar la hip贸tesis nula**: hay homocedasticidad (p > 0.05).\")\n",
        "else:\n",
        "    print(\"   -> **Se rechaza la hip贸tesis nula**: hay heterocedasticidad (p <= 0.05).\")\n",
        "\n",
        "white_test = het_white(residuals_ols, X_const)\n",
        "print(f\"White Test: LM Statistic={white_test[0]:.3f}, p-value={white_test[1]:.3f}, F-statistic={white_test[2]:.3f}, F-p-value={white_test[3]:.3f}\")\n",
        "if white_test[1] > 0.05:\n",
        "    print(\"   -> **No se puede rechazar la hip贸tesis nula**: hay homocedasticidad (p > 0.05).\")\n",
        "else:\n",
        "    print(\"   -> **Se rechaza la hip贸tesis nula**: hay heterocedasticidad (p <= 0.05).\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "## Supuesto 4: Ausencia de Multicolinealidad \n",
        "print(\"\\n--- Evaluaci贸n del Supuesto de Ausencia de Multicolinealidad ---\")\n",
        "if X.ndim > 1 and X.shape[1] > 1: # Solo se aplica si hay m谩s de una variable predictora\n",
        "    print(\"Matriz de Correlaci贸n entre Variables Predictoras:\")\n",
        "    correlation_matrix = X.corr()\n",
        "    print(correlation_matrix)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title('Matriz de Correlaci贸n de Variables Predictoras')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nFactor de Inflaci贸n de la Varianza (VIF):\")\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"feature\"] = X.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "    print(vif_data)\n",
        "\n",
        "    print(\"\\nInterpretaci贸n del VIF:\")\n",
        "    print(\"   - VIF = 1: No hay correlaci贸n.\")\n",
        "    print(\"   - VIF entre 1 y 5: Moderada correlaci贸n.\")\n",
        "    print(\"   - VIF > 5 (o > 10, seg煤n la literatura): Alta multicolinealidad, lo que puede ser problem谩tico.\")\n",
        "else:\n",
        "    print(\"Solo hay una variable predictora (X), por lo que la multicolinealidad no es un problema en este modelo.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "## Supuesto 5: Independencia de los Residuos (Ausencia de Autocorrelaci贸n) \n",
        "print(\"\\n--- Evaluaci贸n del Supuesto de Independencia de los Residuos (OLS) ---\")\n",
        "summary_string_ols = str(results_ols.summary())\n",
        "durbin_watson_line_ols = [line for line in summary_string_ols.split('\\n') if 'Durbin-Watson' in line][0]\n",
        "durbin_watson_stat_ols = float(durbin_watson_line_ols.split()[-1])\n",
        "\n",
        "print(f\"Estad铆stico Durbin-Watson (del resumen del modelo OLS): {durbin_watson_stat_ols:.3f}\")\n",
        "print(\"Interpretaci贸n del Durbin-Watson:\")\n",
        "print(\"   - Valor **cercano a 2**: No hay autocorrelaci贸n.\")\n",
        "print(\"   - Valor **< 2**: Autocorrelaci贸n positiva (residuos adyacentes similares).\")\n",
        "print(\"   - Valor **> 2**: Autocorrelaci贸n negativa (residuos adyacentes opuestos).\")\n",
        "print(\"   - Un valor entre 1.5 y 2.5 generalmente se considera aceptable.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- Implementaci贸n de FGLS para Autocorrelaci贸n (usando GLSAR) ---\n",
        "# GLSAR es una implementaci贸n de GLS para corregir la autocorrelaci贸n.\n",
        "# Estima el par谩metro de autocorrelaci贸n (rho) y luego realiza la transformaci贸n.\n",
        "\n",
        "print(\"\\n--- Implementaci贸n de FGLS para corregir Autocorrelaci贸n (usando GLSAR) ---\")\n",
        "\n",
        "# Para GLSAR, necesitamos un modelo GLSAR y el n煤mero de lags para el AR(p) proceso.\n",
        "# Generalmente, se empieza con AR(1) (lags=1) si el Durbin-Watson indica autocorrelaci贸n positiva.\n",
        "# Si el Durbin-Watson es bajo (mucho menor que 2), sugiere autocorrelaci贸n positiva de primer orden.\n",
        "\n",
        "# Ajustar un modelo GLSAR con un proceso AR(1)\n",
        "model_glsar = GLSAR(y, X_const, 1) # 1 indica AR(1)\n",
        "results_glsar = model_glsar.iterative_fit()\n",
        "\n",
        "print(\"\\nResumen del Modelo de Regresi贸n con FGLS (GLSAR - AR(1)):\")\n",
        "print(results_glsar.summary())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Obtener los residuos y valores predichos del modelo GLSAR\n",
        "residuals_glsar = results_glsar.resid\n",
        "fitted_values_glsar = results_glsar.fittedvalues\n",
        "\n",
        "# --- Re-evaluaci贸n del Supuesto de Independencia de los Residuos (con FGLS) ---\n",
        "print(\"\\n--- Re-evaluaci贸n del Supuesto de Independencia de los Residuos (despu茅s de FGLS) ---\")\n",
        "summary_string_glsar = str(results_glsar.summary())\n",
        "durbin_watson_line_glsar = [line for line in summary_string_glsar.split('\\n') if 'Durbin-Watson' in line][0]\n",
        "durbin_watson_stat_glsar = float(durbin_watson_line_glsar.split()[-1])\n",
        "\n",
        "print(f\"Estad铆stico Durbin-Watson (del resumen del modelo GLSAR): {durbin_watson_stat_glsar:.3f}\")\n",
        "print(\"Interpretaci贸n del Durbin-Watson:\")\n",
        "print(\"   - Valor **cercano a 2**: No hay autocorrelaci贸n.\")\n",
        "print(\"   - Valor **< 2**: Autocorrelaci贸n positiva (residuos adyacentes similares).\")\n",
        "print(\"   - Valor **> 2**: Autocorrelaci贸n negativa (residuos adyacentes opuestos).\")\n",
        "print(\"   - Un valor entre 1.5 y 2.5 generalmente se considera aceptable.\")\n",
        "print(\"\\nDespu茅s de aplicar FGLS, esperamos que el estad铆stico Durbin-Watson se acerque a 2.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- Visualizaci贸n de Residuos despu茅s de FGLS ---\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.scatterplot(x=fitted_values_glsar, y=residuals_glsar)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuos vs. Valores Predichos (GLSAR)')\n",
        "plt.xlabel('Valores Predichos (GLSAR)')\n",
        "plt.ylabel('Residuos (GLSAR)')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(residuals_glsar, kde=True)\n",
        "plt.title('Histograma de Residuos (GLSAR)')\n",
        "plt.xlabel('Residuos (GLSAR)')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Conclusi贸n de la Evaluaci贸n de Supuestos ---\")\n",
        "print(\"La evaluaci贸n de estos supuestos es crucial para confiar en las inferencias del modelo de regresi贸n lineal.\")\n",
        "print(\"Si los supuestos no se cumplen, las estimaciones de los coeficientes y sus errores est谩ndar pueden no ser v谩lidas,\")\n",
        "print(\"afectando la significancia estad铆stica y la capacidad de generalizaci贸n del modelo.\")\n",
        "print(\"En caso de incumplimiento, se pueden considerar transformaciones de datos, la inclusi贸n de otras variables,\")\n",
        "print(\"o el uso de modelos de regresi贸n m谩s avanzados (ej. regresi贸n robusta, modelos generalizados lineales).\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mgNnPb5h4DG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OFIGabzTgbSk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aqcb1OCVfRVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1a4d6fc"
      },
      "source": [
        "##M铆nimos cuadrados ponderados (wls)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5abdd950"
      },
      "source": [
        "# --- Implementaci贸n de M铆nimos Cuadrados Ponderados (WLS) ---\n",
        "\n",
        "print(\"\\n--- Implementaci贸n de M铆nimos Cuadrados Ponderados (WLS) ---\")\n",
        "\n",
        "# 1. Explicaci贸n de WLS\n",
        "print(\"\\n**Concepto de M铆nimos Cuadrados Ponderados (WLS)**\")\n",
        "print(\"M铆nimos Cuadrados Ponderados (WLS) es un m茅todo de regresi贸n que se utiliza\")\n",
        "print(\"cuando el supuesto de homocedasticidad de los errores se viola (es decir, hay heterocedasticidad).\")\n",
        "print(\"La heterocedasticidad implica que la varianza de los errores no es constante\")\n",
        "print(\"a lo largo de los diferentes niveles de las variables predictoras.\")\n",
        "print(\"En WLS, se asignan pesos a cada observaci贸n. Las observaciones con menor varianza (errores m谩s peque帽os)\")\n",
        "print(\"reciben pesos mayores, y las observaciones con mayor varianza (errores m谩s grandes)\")\n",
        "print(\"reciben pesos menores. Esto le da m谩s 'importancia' en el ajuste del modelo a las observaciones\")\n",
        "print(\"que son m谩s fiables (tienen menor varianza), corrigiendo as铆 el problema de la heterocedasticidad\")\n",
        "print(\"y produciendo estimaciones de coeficientes m谩s eficientes (menores errores est谩ndar).\")\n",
        "print(\"Generalmente, los pesos son inversamente proporcionales a la varianza de los errores.\")\n",
        "\n",
        "# 2. Definir los pesos\n",
        "# Una forma com煤n de estimar los pesos es usar los residuos del modelo OLS inicial.\n",
        "# Si la varianza de los errores es proporcional a alguna funci贸n de las variables predictoras\n",
        "# o de los valores ajustados, podemos usar esa funci贸n para definir los pesos.\n",
        "# Por ejemplo, si la varianza es proporcional a los valores ajustados al cuadrado (fitted_values_ols**2),\n",
        "# los pesos ser铆an inversamente proporcionales a fitted_values_ols**2.\n",
        "# Para evitar dividir por cero o valores muy peque帽os, a menudo se a帽ade una peque帽a constante.\n",
        "\n",
        "# Estimaci贸n de los pesos: Inverso de la varianza estimada de los residuos OLS\n",
        "# Una aproximaci贸n simple es usar el inverso de los residuos al cuadrado del modelo OLS.\n",
        "# Sin embargo, esto puede ser problem谩tico si los residuos son cercanos a cero.\n",
        "# Una alternativa com煤n es usar el inverso de los valores ajustados al cuadrado o alguna transformaci贸n de X.\n",
        "# Aqu铆, usaremos el inverso de los valores ajustados del modelo OLS.\n",
        "# Se recomienda a帽adir una peque帽a constante para evitar la divisi贸n por cero.\n",
        "weights = 1.0 / (fitted_values_ols**2 + 1e-6) # A帽adir una peque帽a constante para estabilidad\n",
        "\n",
        "print(\"\\nPesos calculados (basados en el inverso de los valores ajustados del modelo OLS):\")\n",
        "print(weights.head())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 3. Ajustar el modelo WLS\n",
        "# Usamos la clase WLS de statsmodels, pasando la variable dependiente, las predictoras con constante, y los pesos.\n",
        "model_wls = sm.WLS(y, X_const, weights=weights)\n",
        "results_wls = model_wls.fit()\n",
        "\n",
        "# 4. Mostrar el resumen de los resultados del modelo WLS\n",
        "print(\"\\nResumen del Modelo de Regresi贸n con WLS:\")\n",
        "print(results_wls.summary())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 5. Explicar c贸mo los pesos influyen en el ajuste y los resultados\n",
        "print(\"\\n**Influencia de los Pesos en el Modelo WLS**\")\n",
        "print(\"En el modelo WLS, las observaciones con pesos m谩s altos (generalmente aquellas con menor varianza de error estimada)\")\n",
        "print(\"tienen una mayor influencia en la determinaci贸n de los coeficientes de regresi贸n.\")\n",
        "print(\"Esto significa que el modelo se ajusta m谩s estrechamente a los puntos de datos que se consideran m谩s 'fiables' (menos ruidosos).\")\n",
        "print(\"La principal ventaja es que las estimaciones de los coeficientes son m谩s eficientes (tienen menores errores est谩ndar)\")\n",
        "print(\"en presencia de heterocedasticidad, en comparaci贸n con OLS.\")\n",
        "print(\"Comparando el resumen de WLS con el de OLS, se observar谩 que los errores est谩ndar de los coeficientes son diferentes,\")\n",
        "print(\"y potencialmente menores, lo que puede afectar la significancia estad铆stica (valores p).\")\n",
        "print(\"El R-cuadrado en WLS se interpreta de manera diferente a OLS y a menudo no es directamente comparable.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# Obtener los residuos y valores predichos del modelo WLS\n",
        "residuals_wls = results_wls.resid\n",
        "fitted_values_wls = results_wls.fittedvalues\n",
        "\n",
        "# 6. Incluir visualizaciones (residuos ponderados vs. valores ajustados)\n",
        "# Para evaluar la homocedasticidad despu茅s de aplicar WLS, es 煤til graficar los residuos ponderados\n",
        "# (residuos * sqrt(weights)) o los residuos sin ponderar frente a los valores ajustados.\n",
        "# Idealmente, los residuos ponderados vs. valores ajustados deber铆an mostrar una dispersi贸n m谩s uniforme alrededor de cero.\n",
        "\n",
        "# Calcular residuos ponderados\n",
        "weighted_residuals_wls = residuals_wls * np.sqrt(weights)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Gr谩fico de Residuos WLS sin ponderar vs. Valores Predichos WLS\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.scatterplot(x=fitted_values_wls, y=residuals_wls)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuos (WLS) vs. Valores Predichos (WLS)')\n",
        "plt.xlabel('Valores Predichos (WLS)')\n",
        "plt.ylabel('Residuos (WLS)')\n",
        "\n",
        "# Gr谩fico de Residuos Ponderados WLS vs. Valores Predichos WLS\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.scatterplot(x=fitted_values_wls, y=weighted_residuals_wls)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuos Ponderados (WLS) vs. Valores Predichos (WLS)')\n",
        "plt.xlabel('Valores Predichos (WLS)')\n",
        "plt.ylabel('Residuos Ponderados (WLS)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Evaluaci贸n de Homocedasticidad (despu茅s de WLS) ---\")\n",
        "print(\"Observar el gr谩fico de 'Residuos Ponderados (WLS) vs. Valores Predichos (WLS)'.\")\n",
        "print(\"Esperamos que la dispersi贸n de los puntos sea m谩s uniforme alrededor de la l铆nea cero\")\n",
        "print(\"en comparaci贸n con el gr谩fico de residuos vs. valores predichos del modelo OLS inicial.\")\n",
        "print(\"Esto indicar铆a que WLS ha ayudado a corregir la heterocedasticidad.\")\n",
        "print(\"-\" * 50)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2bfee15"
      },
      "source": [
        "## Regresi贸n Lineal Robusta\n",
        ".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "059068ac"
      },
      "source": [
        "# --- Implementaci贸n de Regresi贸n Lineal Robusta ---\n",
        "\n",
        "print(\"\\n--- Implementaci贸n de Regresi贸n Lineal Robusta ---\")\n",
        "\n",
        "# 1. Explicaci贸n de Regresi贸n Lineal Robusta\n",
        "print(\"\\n**Concepto de Regresi贸n Lineal Robusta**\")\n",
        "print(\"La Regresi贸n Lineal Robusta es una alternativa a M铆nimos Cuadrados Ordinarios (OLS)\")\n",
        "print(\"que es menos sensible a la presencia de valores at铆picos (outliers) o influyentes\")\n",
        "print(\"en los datos. Mientras que OLS minimiza la suma de los errores al cuadrado,\")\n",
        "print(\"lo que hace que los valores at铆picos tengan un impacto cuadr谩tico grande en el ajuste,\")\n",
        "print(\"la regresi贸n robusta utiliza diferentes funciones de p茅rdida o esquemas de ponderaci贸n\")\n",
        "print(\"para mitigar la influencia de estas observaciones extremas.\")\n",
        "print(\"Se utiliza t铆picamente cuando la inspecci贸n visual de los datos o el an谩lisis de residuos\")\n",
        "print(\"del modelo OLS inicial sugiere la presencia de outliers que podr铆an distorsionar los resultados de OLS.\")\n",
        "\n",
        "# 2. M茅todos de Regresi贸n Robusta en statsmodels\n",
        "print(\"\\n**M茅todos de Regresi贸n Robusta en `statsmodels`**\")\n",
        "print(\"`statsmodels` implementa varios m茅todos de estimaci贸n robusta.\")\n",
        "print(\"Los m谩s comunes son los M-estimators (Estimadores M).\")\n",
        "print(\"Los M-estimators generalizan la idea de OLS minimizando una funci贸n de p茅rdida (rho)\")\n",
        "print(\"de los residuos, en lugar del cuadrado de los residuos. Equivalente a esto,\")\n",
        "print(\"ponderan las observaciones de manera iterativa en funci贸n del tama帽o de sus residuos;\")\n",
        "print(\"los residuos m谩s grandes reciben un peso menor.\")\n",
        "print(\"Algunas funciones de ponderaci贸n (normas) comunes para M-estimators incluyen:\")\n",
        "print(\"  - Huber's T (HuberT): Pondera linealmente los residuos grandes.\")\n",
        "print(\"  - Tukey's Biweight (TukeyBiweight): Asigna peso cero a residuos que exceden un cierto umbral,\")\n",
        "print(\"    eliminando efectivamente su influencia.\")\n",
        "print(\"La clase `statsmodels.api.RLM` (Robust Linear Model) implementa M-estimators.\")\n",
        "print(\"Por defecto, `RLM` utiliza la norma de Huber (HuberT).\")\n",
        "\n",
        "# 3. Importar la clase RLM (ya importada en una celda anterior)\n",
        "# from statsmodels.api import RLM # No es necesario importar de nuevo\n",
        "\n",
        "# 4. Instanciar un modelo de regresi贸n robusta\n",
        "# Usamos la clase RLM de statsmodels. Podemos especificar una funci贸n de ponderaci贸n (norm).\n",
        "# Por defecto, usa HuberT. Aqu铆 usaremos HuberT expl铆citamente o dejaremos el valor por defecto.\n",
        "# Tambi茅n podemos usar TukeyBiweight: sm.robust.norms.TukeyBiweight()\n",
        "\n",
        "model_robust = sm.RLM(y, X_const, M=sm.robust.norms.HuberT())\n",
        "# O simplemente: model_robust = sm.RLM(y, X_const) # Usa HuberT por defecto\n",
        "\n",
        "# 5. Ajustar el modelo robusto\n",
        "results_robust = model_robust.fit()\n",
        "\n",
        "# 6. Mostrar el resumen de los resultados del modelo de regresi贸n robusta\n",
        "print(\"\\nResumen del Modelo de Regresi贸n Robusta (usando HuberT):\")\n",
        "print(results_robust.summary())\n",
        "print(\"-\" * 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1f466cf"
      },
      "source": [
        "# 7. Obtener los residuos y valores predichos del modelo robusto\n",
        "residuals_robust = results_robust.resid\n",
        "fitted_values_robust = results_robust.fittedvalues\n",
        "\n",
        "print(\"\\nResiduos del modelo Robusto (HuberT):\")\n",
        "print(residuals_robust.head())\n",
        "print(\"\\nValores predichos del modelo Robusto (HuberT):\")\n",
        "print(fitted_values_robust.head())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 8. Visualizar los residuos del modelo robusto\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Gr谩fico de Residuos Robusto vs. Valores Predichos Robusto\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.scatterplot(x=fitted_values_robust, y=residuals_robust)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuos (Robusto - HuberT) vs. Valores Predichos (Robusto)')\n",
        "plt.xlabel('Valores Predichos (Robusto)')\n",
        "plt.ylabel('Residuos (Robusto)')\n",
        "\n",
        "# Gr谩fico de Residuos OLS vs. Valores Predichos OLS para comparaci贸n (usando variables de celdas anteriores)\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.scatterplot(x=fitted_values_ols, y=residuals_ols)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuos (OLS) vs. Valores Predichos (OLS) - Comparaci贸n')\n",
        "plt.xlabel('Valores Predichos (OLS)')\n",
        "plt.ylabel('Residuos (OLS)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Evaluaci贸n Visual de Residuos (Robusto vs. OLS) ---\")\n",
        "print(\"Observar el gr谩fico de 'Residuos (Robusto - HuberT) vs. Valores Predichos (Robusto)'.\")\n",
        "print(\"La dispersi贸n de los puntos puede ser diferente a la del gr谩fico OLS,\")\n",
        "print(\"especialmente si hay valores at铆picos. Los puntos at铆picos que ten铆an\")\n",
        "print(\"una gran desviaci贸n en OLS pueden tener una menor influencia o un patr贸n diferente\")\n",
        "print(\"en el modelo robusto debido a la reponderaci贸n.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# 9. Imprimir una breve interpretaci贸n de los resultados del modelo robusto\n",
        "print(\"\\n--- Interpretaci贸n de los Resultados del Modelo Robusto (HuberT) ---\")\n",
        "print(\"Comparando el resumen del Modelo Robusto con el del Modelo OLS inicial:\")\n",
        "print(f\"- Coeficiente 'const' (Robusto): {results_robust.params['const']:.3f} (OLS: {results_ols.params['const']:.3f})\")\n",
        "print(f\"- Coeficiente 'Horas(x)' (Robusto): {results_robust.params['Horas(x)']:.3f} (OLS: {results_ols.params['Horas(x)']:.3f})\")\n",
        "print(f\"- Error Est谩ndar 'const' (Robusto): {results_robust.bse['const']:.3f} (OLS: {results_ols.bse['const']:.3f})\")\n",
        "print(f\"- Error Est谩ndar 'Horas(x)' (Robusto): {results_robust.bse['Horas(x)']:.3f} (OLS: {results_ols.bse['Horas(x)']:.3f})\")\n",
        "\n",
        "print(\"\\nObservaciones:\")\n",
        "print(\"Los coeficientes estimados por el modelo robusto pueden diferir de los de OLS,\")\n",
        "print(\"especialmente si hay valores at铆picos que influyen en el ajuste de OLS.\")\n",
        "print(\"Los errores est谩ndar del modelo robusto a menudo se calculan de manera diferente\")\n",
        "print(\"(por defecto, usa Cov Type H1 en statsmodels RLM, que es robusta a la heterocedasticidad)\")\n",
        "print(\"y pueden proporcionar inferencias m谩s fiables en presencia de desviaciones de los supuestos de OLS.\")\n",
        "print(\"La regresi贸n robusta, al ponderar menos los outliers, tiende a ajustar la 'tendencia central' de los datos,\")\n",
        "print(\"siendo menos arrastrada por puntos extremos.\")\n",
        "print(\"En este caso, el coeficiente de 'Horas(x)' es ligeramente diferente y el intercepto tambi茅n,\")\n",
        "print(\"lo que sugiere que el modelo robusto ha ajustado el ajuste para ser menos influenciado por\")\n",
        "print(\"las observaciones con residuos grandes (los posibles outliers identificados en el an谩lisis OLS anterior).\")\n",
        "print(\"-\" * 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "862127ae"
      },
      "source": [
        "## Modelos Lineales Generalizados\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e03f43b3"
      },
      "source": [
        "# --- Implementaci贸n de Modelos Lineales Generalizados (GLM) ---\n",
        "\n",
        "print(\"\\n--- Implementaci贸n de Modelos Lineales Generalizados (GLM) ---\")\n",
        "\n",
        "# 1. Explicaci贸n de GLM\n",
        "print(\"\\n**Concepto de Modelos Lineales Generalizados (GLM)**\")\n",
        "print(\"Los Modelos Lineales Generalizados (GLM) extienden el modelo de regresi贸n lineal cl谩sico (OLS)\")\n",
        "print(\"para permitir que la variable dependiente (respuesta) tenga una distribuci贸n de error\")\n",
        "print(\"distinta a la normal y para relacionar la media de la respuesta con una combinaci贸n\")\n",
        "print(\"lineal de las variables predictoras a trav茅s de una **funci贸n de enlace**.\")\n",
        "print(\"El modelo lineal cl谩sico asume que:\")\n",
        "print(\"  - La variable dependiente es continua.\")\n",
        "print(\"  - Los errores son independientes y se distribuyen normalmente con media cero y varianza constante (homocedasticidad).\")\n",
        "print(\"  - La relaci贸n entre la media de la variable dependiente y las predictoras es lineal.\")\n",
        "print(\"\\nGLM relaja estas suposiciones al especificar:\")\n",
        "print(\"  - Una **distribuci贸n de probabilidad** para la variable dependiente (ej. Binomial para datos binarios, Poisson para conteos, Gamma para datos positivos asim茅tricos).\")\n",
        "print(\"  - Una **funci贸n de enlace** que transforma la media esperada de la variable dependiente para que la relaci贸n con la combinaci贸n lineal de las predictoras sea lineal. G(E[y]) = X尾.\")\n",
        "print(\"  - Una **funci贸n de varianza** que describe c贸mo la varianza de la respuesta depende de la media.\")\n",
        "\n",
        "# 2. Ejemplos comunes de GLM\n",
        "print(\"\\n**Ejemplos Comunes de GLM:**\")\n",
        "print(\"  - **Regresi贸n Log铆stica:** Para variables dependientes binarias (0/1). Utiliza la familia Binomial y la funci贸n de enlace logit.\")\n",
        "print(\"  - **Regresi贸n de Poisson:** Para variables dependientes de conteo (n煤meros enteros no negativos). Utiliza la familia Poisson y la funci贸n de enlace log.\")\n",
        "print(\"  - **Regresi贸n Gamma:** Para variables dependientes continuas y positivas con distribuciones asim茅tricas (ej. tiempos de espera, reclamaciones de seguros). Utiliza la familia Gamma y una funci贸n de enlace como la inversa o la log.\")\n",
        "print(\"  - **Regresi贸n Lineal (como un caso especial de GLM):** Utiliza la familia Gaussiana (Normal) y la funci贸n de enlace identidad.\")\n",
        "\n",
        "\n",
        "# 3. Preparar datos para un ejemplo b谩sico de GLM (Regresi贸n de Poisson)\n",
        "# El conjunto de datos actual ('data') es continuo y no es ideal para Poisson o Log铆stica.\n",
        "# Simularemos datos simples para una Regresi贸n de Poisson.\n",
        "# Supongamos que queremos modelar el n煤mero de eventos (un conteo) en funci贸n de una variable predictora.\n",
        "\n",
        "np.random.seed(42) # para reproducibilidad\n",
        "n_samples = 100\n",
        "# Variable predictora (ej. nivel de exposici贸n, tiempo)\n",
        "X_sim = np.random.rand(n_samples) * 10\n",
        "# Combinaci贸n lineal de predictoras (en este caso, solo una)\n",
        "linear_predictor = 0.5 + 0.3 * X_sim\n",
        "# Media esperada para una distribuci贸n de Poisson (lambda = exp(linear_predictor))\n",
        "# La funci贸n de enlace log (log(lambda) = linear_predictor) es la can贸nica para Poisson.\n",
        "mu_sim = np.exp(linear_predictor)\n",
        "# Generar datos de conteo siguiendo una distribuci贸n de Poisson con la media calculada\n",
        "y_sim = np.random.poisson(mu_sim)\n",
        "\n",
        "# Crear un DataFrame para los datos simulados\n",
        "data_glm = pd.DataFrame({'X_sim': X_sim, 'y_sim': y_sim})\n",
        "\n",
        "# A帽adir una constante a las caracter铆sticas para el modelo statsmodels\n",
        "X_sim_const = sm.add_constant(data_glm['X_sim'])\n",
        "y_sim = data_glm['y_sim']\n",
        "\n",
        "print(\"\\nDatos Simulados para Regresi贸n de Poisson:\")\n",
        "print(data_glm.head())\n",
        "print(\"-\" * 50)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19143f6d"
      },
      "source": [
        "# 4. Instanciar un modelo GLM (Regresi贸n de Poisson)\n",
        "# Usamos la clase GLM de statsmodels, especificando la variable dependiente, las predictoras con constante,\n",
        "# y la familia de distribuci贸n de errores (Poisson en este caso).\n",
        "\n",
        "model_glm = sm.GLM(y_sim, X_sim_const, family=sm.families.Poisson())\n",
        "\n",
        "# 5. Ajustar el modelo GLM\n",
        "results_glm = model_glm.fit()\n",
        "\n",
        "# 6. Mostrar el resumen de los resultados del modelo GLM\n",
        "print(\"\\nResumen del Modelo GLM (Regresi贸n de Poisson):\")\n",
        "print(results_glm.summary())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 7. Explicar las diferencias clave en la interpretaci贸n de los resultados de un GLM\n",
        "print(\"\\n**Diferencias Clave en la Interpretaci贸n de Resultados (GLM vs. OLS)**\")\n",
        "print(\"Comparando el resumen del Modelo GLM (Poisson) con el del Modelo OLS:\")\n",
        "print(\"\\n**Coeficientes:**\")\n",
        "print(\"  - En OLS, los coeficientes se interpretan directamente como el cambio esperado en la media de Y por un cambio de una unidad en X, manteniendo otras variables constantes.\")\n",
        "print(\"  - En GLM, los coeficientes se interpretan en la **escala de la funci贸n de enlace**. Para la Regresi贸n de Poisson con enlace log, el coeficiente de una predictora\")\n",
        "print(\"    representa el cambio esperado en el **logaritmo de la media** de Y por un cambio de una unidad en X.\")\n",
        "print(\"    Esto significa que un cambio de una unidad en X multiplica la media esperada de Y por exp(coeficiente).\")\n",
        "print(\"    Por ejemplo, si el coeficiente es 0.3, un aumento de 1 en X multiplica la media esperada por exp(0.3)  1.35 (un aumento del 35%).\")\n",
        "\n",
        "print(\"\\n**Bondad de Ajuste:**\")\n",
        "print(\"  - OLS utiliza R-cuadrado como m茅trica principal. Mide la proporci贸n de la varianza en Y explicada por el modelo.\")\n",
        "print(\"  - GLM no usa R-cuadrado de la misma manera. Las m茅tricas de bondad de ajuste para GLM a menudo se basan en la **devianza**, que es una generalizaci贸n de la suma de cuadrados residuales.\")\n",
        "print(\"    La devianza residual mide la discrepancia entre el modelo ajustado y los datos saturados (un modelo que ajusta perfectamente cada observaci贸n).\")\n",
        "print(\"    Una devianza residual peque帽a en relaci贸n con los grados de libertad sugiere un buen ajuste.\")\n",
        "print(\"    Tambi茅n se puede usar la devianza nula (deviance of the null model, con solo el intercepto) para calcular m茅tricas an谩logas al R-cuadrado (pseudo R-squared),\")\n",
        "print(\"    como el Pseudo R-squared de McFadden, Cox-Snell, o Nagelkerke, aunque su interpretaci贸n es diferente a la de OLS.\")\n",
        "print(\"    El AIC (Criterio de Informaci贸n de Akaike) y BIC (Criterio de Informaci贸n Bayesiano) tambi茅n se utilizan para comparar modelos GLM.\")\n",
        "\n",
        "print(\"\\n**Supuestos:**\")\n",
        "print(\"  - OLS asume normalidad, homocedasticidad e independencia de los errores.\")\n",
        "print(\"  - GLM asume que la variable dependiente sigue la distribuci贸n de la familia especificada y que los errores son independientes.\")\n",
        "print(\"    No asume homocedasticidad (la varianza puede depender de la media, seg煤n la familia) ni normalidad de los errores en la escala original.\")\n",
        "\n",
        "print(\"\\n**Errores Est谩ndar y Significancia:**\")\n",
        "print(\"  - Los errores est谩ndar y los valores p se interpretan de manera similar (para probar la significancia de los coeficientes),\")\n",
        "print(\"    pero se derivan de la teor铆a de m谩xima verosimilitud, que es la base de la estimaci贸n de GLM, en lugar de OLS.\")\n",
        "\n",
        "# 8. Concluir resaltando cu谩ndo usar GLM\n",
        "print(\"\\n--- Cu谩ndo Considerar el Uso de un GLM ---\")\n",
        "print(\"Debe considerar usar un GLM en lugar de OLS cuando:\")\n",
        "print(\"  - Su variable dependiente **no es continua** o **no tiene errores distribuidos normalmente**.\")\n",
        "print(\"    (ej. binaria, conteo, proporci贸n, datos positivos asim茅tricos).\")\n",
        "print(\"  - La **varianza de los errores no es constante** y/o depende de la media de la respuesta.\")\n",
        "print(\"  - La relaci贸n entre la media de la respuesta y las predictoras **no es lineal** en la escala original,\")\n",
        "print(\"    pero puede ser lineal en la escala transformada por una funci贸n de enlace adecuada.\")\n",
        "print(\"GLM proporciona un marco flexible para modelar una variedad m谩s amplia de tipos de datos de respuesta que OLS.\")\n",
        "print(\"-\" * 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c21148b"
      },
      "source": [
        "La Regresi贸n por M铆nimos Cuadrados Ponderados (WLS) aborda la heterocedasticidad asignando pesos a las observaciones, dando m谩s importancia a aquellas con menor varianza de error. Un enfoque com煤n es usar pesos inversamente proporcionales a los valores ajustados al cuadrado de un modelo OLS inicial. El resumen del modelo WLS muestra errores est谩ndar ajustados en comparaci贸n con OLS, y los gr谩ficos de residuos ponderados pueden indicar si la heterocedasticidad se ha mitigado.\n",
        "\n",
        "Los M铆nimos Cuadrados Generalizados (GLS) son un m茅todo m谩s general que OLS y WLS, capaz de manejar tanto la heterocedasticidad como la autocorrelaci贸n. Su implementaci贸n en statsmodels requiere especificar la matriz de covarianza de error (惟). Si 惟 es diagonal con entradas inversamente proporcionales a los pesos de WLS, los resultados de GLS ser谩n num茅ricamente id茅nticos a WLS. En la pr谩ctica, 惟 es a menudo desconocida y necesita ser estimada (GLS Factible - FGLS).\n",
        "\n",
        "La Regresi贸n Lineal Robusta es menos sensible a los valores at铆picos que OLS. Los estimadores M, como Huber's T o Tukey's Biweight, son m茅todos comunes que re-ponderan iterativamente las observaciones bas谩ndose en sus residuos, reduciendo la influencia de los residuos grandes. El resumen del modelo robusto y los gr谩ficos de residuos pueden mostrar diferencias en las estimaciones de los coeficientes y los errores est谩ndar de los errores en comparaci贸n con OLS, lo que refleja el impacto reducido de los valores at铆picos.\n",
        "\n",
        "Los Modelos Lineales Generalizados (GLM) extienden la regresi贸n lineal a variables dependientes que no son continuas o normalmente distribuidas (por ejemplo, binarias, de recuento). Los GLM requieren especificar una distribuci贸n de probabilidad para la respuesta (por ejemplo, Binomial, Poisson) y una funci贸n de enlace para relacionar la media de la respuesta con el predictor lineal. La interpretaci贸n de los coeficientes se realiza en la escala de la funci贸n de enlace, no en la escala de respuesta original, y la bondad del ajuste se eval煤a utilizando m茅tricas como la deviance y el pseudo R-cuadrado en lugar del R-cuadrado de OLS.\n",
        "\n"
      ]
    }
  ]
}