{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NatSama2/Bootcamp-Analisis-de-Datos/blob/main/Modulo-5/Regresi%C3%B3n_Puntaje_Horas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regresión Lineal Simple MCO"
      ],
      "metadata": {
        "id": "tBLHSoYlqQZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import probplot, shapiro, jarque_bera, normaltest, anderson, kstest\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan, het_white,lilliefors\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.regression.linear_model import GLS, GLSAR\n"
      ],
      "metadata": {
        "id": "QLwIOX46G7Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(\"/content/sample_data/Data_Regresion.xlsx\", sheet_name='data')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "YCcNF3LEmiBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTWmZ76Oj1rF"
      },
      "outputs": [],
      "source": [
        "# Este Script sera modificado para tu conveniencia\n",
        "\n",
        "#1 Definir variables dependiente (Y) e independiente (X)\n",
        "\n",
        "X = data['Horas(x)']\n",
        "y= data['Puntaje(y)']\n",
        "\n",
        "# Añadir una constante a las características para el modelo statsmodels\n",
        "X_const = sm.add_constant(X)\n",
        "\n",
        "print(\"Datos Cargados:\")\n",
        "print(data)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 2. Construcción del Modelo de Regresión Lineal ---\n",
        "# Usamos statsmodels para tener un resumen detallado y acceso a los residuos para los supuestos.\n",
        "model = sm.OLS(y, X_const)\n",
        "## results = model.fit(cov_type='HAC', cov_kwds={'maxlags':1})\n",
        "results = model.fit()\n",
        "print(\"Resumen del Modelo de Regresión Lineal:\")\n",
        "print(results.summary())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Obtener los residuos del modelo\n",
        "residuals = results.resid\n",
        "fitted_values = results.fittedvalues\n",
        "\n",
        "# --- 3. Evaluación de Supuestos de la Regresión Lineal ---\n",
        "\n",
        "## Supuesto 1: Linealidad 📈\n",
        "# Se asume que la relación entre las variables es lineal.\n",
        "# Se puede verificar con un gráfico de dispersión de la variable predictora contra la variable objetivo,\n",
        "# y también con un gráfico de residuos vs. valores predichos.\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Gráfico de dispersión de X vs. Y\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.scatterplot(x=X, y=y)\n",
        "plt.title('Linealidad: X vs. Y')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "\n",
        "# Residuos vs. Valores Predichos: La mejor manera de verificar la linealidad y homocedasticidad\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.scatterplot(x=fitted_values, y=residuals)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuos vs. Valores Predichos')\n",
        "plt.xlabel('Valores Predichos')\n",
        "plt.ylabel('Residuos')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Evaluación del Supuesto de Linealidad ---\")\n",
        "print(\"Observar el gráfico 'X vs. Y': si los puntos siguen una tendencia lineal, el supuesto se cumple.\")\n",
        "print(\"En el gráfico de 'Residuos vs. Valores Predichos', los residuos deben estar distribuidos aleatoriamente alrededor de cero, sin patrones evidentes (forma de embudo, curva, etc.).\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "## Supuesto 2: Normalidad de los Residuos 📊\n",
        "# Los residuos deben seguir una distribución normal.\n",
        "# Se puede verificar con gráficos (histograma, Q-Q plot) y pruebas estadísticas.\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Histograma de los residuos\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.title('Histograma de los Residuos')\n",
        "plt.xlabel('Residuos')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "# Q-Q plot de los residuos\n",
        "plt.subplot(1, 2, 2)\n",
        "probplot(residuals, dist=\"norm\", plot=plt)\n",
        "plt.title('Q-Q Plot de los Residuos')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Evaluación del Supuesto de Normalidad de los Residuos ---\")\n",
        "print(\"Pruebas estadísticas:\")\n",
        "\n",
        "# Prueba de Shapiro-Wilk (para n < 50)\n",
        "shapiro_test = shapiro(residuals)\n",
        "print(f\"Shapiro-Wilk Test: Estadístico={shapiro_test.statistic:.3f}, p-value={shapiro_test.pvalue:.3f}\")\n",
        "if shapiro_test.pvalue > 0.05:\n",
        "    print(\"  -> **No se puede rechazar la hipótesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"  -> **Se rechaza la hipótesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "# Prueba de Jarque-Bera (para n > 2000, pero también útil para n pequeñas)\n",
        "jarque_bera_test = jarque_bera(residuals)\n",
        "print(f\"Jarque-Bera Test: Estadístico={jarque_bera_test.statistic:.3f}, p-value={jarque_bera_test.pvalue:.3f}\")\n",
        "if jarque_bera_test.pvalue > 0.05:\n",
        "    print(\"  -> **No se puede rechazar la hipótesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"  -> **Se rechaza la hipótesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "\n",
        "\n",
        "# Prueba de Agostino-Pearson (D'Agostino's K-squared test)\n",
        "agostino_test = normaltest(residuals)\n",
        "print(f\"\\nAgostino-Pearson Test: Estadístico={agostino_test.statistic:.3f}, p-value={agostino_test.pvalue:.3f}\")\n",
        "if agostino_test.pvalue > 0.05:\n",
        "    print(\"  -> **No se puede rechazar la hipótesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"  -> **Se rechaza la hipótesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "# Prueba de Anderson-Darling\n",
        "anderson_test = anderson(residuals, dist='norm')\n",
        "print(f\"\\nAnderson-Darling Test: Estadístico={anderson_test.statistic:.3f}\")\n",
        "print(\"Valores críticos y niveles de significancia:\")\n",
        "for i in range(len(anderson_test.critical_values)):\n",
        "    sl, cv = anderson_test.significance_level[i], anderson_test.critical_values[i]\n",
        "    if anderson_test.statistic < cv:\n",
        "        print(f\"  -> Para nivel de significancia {sl}%: estadístico < {cv:.3f} (No rechazar normalidad)\")\n",
        "    else:\n",
        "        print(f\"  -> Para nivel de significancia {sl}%: estadístico >= {cv:.3f} (Rechazar normalidad)\")\n",
        "\n",
        "# Prueba de Kolmogorov-Smirnov (con parámetros estimados)\n",
        "ks_test = kstest(residuals, 'norm', args=(residuals.mean(), residuals.std()))\n",
        "print(f\"\\nKolmogorov-Smirnov Test: Estadístico={ks_test.statistic:.3f}, p-value={ks_test.pvalue:.3f}\")\n",
        "if ks_test.pvalue > 0.05:\n",
        "    print(\"  -> **No se puede rechazar la hipótesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"  -> **Se rechaza la hipótesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "# Prueba de Lilliefors (Kolmogorov-Smirnov modificada para normalidad con parámetros estimados)\n",
        "lillie_test = lilliefors(residuals, dist='norm')\n",
        "print(f\"\\nLilliefors Test: Estadístico={lillie_test[0]:.3f}, p-value={lillie_test[1]:.3f}\")\n",
        "if lillie_test[1] > 0.05:\n",
        "    print(\"  -> **No se puede rechazar la hipótesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"  -> **Se rechaza la hipótesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nObservar el Q-Q Plot: los puntos deben seguir aproximadamente la línea recta. Las desviaciones indican no normalidad.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "## Supuesto 3: Homocedasticidad (Varianza Constante de los Residuos) 📏\n",
        "# La varianza de los residuos debe ser constante en todos los niveles de las variables predictoras.\n",
        "# Se verifica principalmente con el gráfico de residuos vs. valores predichos y pruebas estadísticas.\n",
        "\n",
        "print(\"\\n--- Evaluación del Supuesto de Homocedasticidad ---\")\n",
        "# El gráfico de residuos vs. valores predichos ya se mostró en la sección de linealidad.\n",
        "# Buscar patrones en forma de embudo o cono, que indicarían heterocedasticidad.\n",
        "\n",
        "print(\"Pruebas estadísticas:\")\n",
        "\n",
        "# Prueba de Breusch-Pagan\n",
        "bp_test = het_breuschpagan(residuals, X_const)\n",
        "print(f\"Breusch-Pagan Test: LM Statistic={bp_test[0]:.3f}, p-value={bp_test[1]:.3f}, F-statistic={bp_test[2]:.3f}, F-p-value={bp_test[3]:.3f}\")\n",
        "if bp_test[1] > 0.05:\n",
        "    print(\"  -> **No se puede rechazar la hipótesis nula**: hay homocedasticidad (p > 0.05).\")\n",
        "else:\n",
        "    print(\"  -> **Se rechaza la hipótesis nula**: hay heterocedasticidad (p <= 0.05).\")\n",
        "\n",
        "# Prueba de White\n",
        "white_test = het_white(residuals, X_const)\n",
        "print(f\"White Test: LM Statistic={white_test[0]:.3f}, p-value={white_test[1]:.3f}, F-statistic={white_test[2]:.3f}, F-p-value={white_test[3]:.3f}\")\n",
        "if white_test[1] > 0.05:\n",
        "    print(\"  -> **No se puede rechazar la hipótesis nula**: hay homocedasticidad (p > 0.05).\")\n",
        "else:\n",
        "    print(\"  -> **Se rechaza la hipótesis nula**: hay heterocedasticidad (p <= 0.05).\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "## Supuesto 4: Ausencia de Multicolinealidad 🔄\n",
        "# Las variables predictoras no deben estar altamente correlacionadas entre sí.\n",
        "# Para este conjunto de datos, solo hay una variable predictora (X), por lo que la multicolinealidad no es un problema.\n",
        "# Sin embargo, se incluye la sección para demostrar cómo se haría con múltiples variables.\n",
        "\n",
        "print(\"\\n--- Evaluación del Supuesto de Ausencia de Multicolinealidad ---\")\n",
        "if X.ndim > 1 and X.shape[1] > 1: # Solo se aplica si hay más de una variable predictora\n",
        "    print(\"Matriz de Correlación entre Variables Predictoras:\")\n",
        "    correlation_matrix = X.corr()\n",
        "    print(correlation_matrix)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title('Matriz de Correlación de Variables Predictoras')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nFactor de Inflación de la Varianza (VIF):\")\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"feature\"] = X.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "    print(vif_data)\n",
        "\n",
        "    print(\"\\nInterpretación del VIF:\")\n",
        "    print(\"  - VIF = 1: No hay correlación.\")\n",
        "    print(\"  - VIF entre 1 y 5: Moderada correlación.\")\n",
        "    print(\"  - VIF > 5 (o > 10, según la literatura): Alta multicolinealidad, lo que puede ser problemático.\")\n",
        "else:\n",
        "    print(\"Solo hay una variable predictora (X), por lo que la multicolinealidad no es un problema en este modelo.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "## Supuesto 5: Independencia de los Residuos (Ausencia de Autocorrelación) 🔗\n",
        "# Los residuos no deben estar correlacionados entre sí. Esto es crucial en series de tiempo.\n",
        "# Se verifica con el estadístico de Durbin-Watson, que está incluido en el resumen de `statsmodels`.\n",
        "\n",
        "print(\"\\n--- Evaluación del Supuesto de Independencia de los Residuos ---\")\n",
        "# Extrae la estadistica de Durbiny-Watson\n",
        "summary_string = str(results.summary())\n",
        "durbin_watson_line = [line for line in summary_string.split('\\n') if 'Durbin-Watson' in line][0]\n",
        "durbin_watson_stat = float(durbin_watson_line.split()[-1])\n",
        "\n",
        "print(f\"Estadístico Durbin-Watson (del resumen del modelo): {durbin_watson_stat:.3f}\")\n",
        "print(\"Interpretación del Durbin-Watson:\")\n",
        "print(\"  - Valor **cercano a 2**: No hay autocorrelación.\")\n",
        "print(\"  - Valor **< 2**: Autocorrelación positiva (residuos adyacentes similares).\")\n",
        "print(\"  - Valor **> 2**: Autocorrelación negativa (residuos adyacentes opuestos).\")\n",
        "print(\"  - Un valor entre 1.5 y 2.5 generalmente se considera aceptable.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"\\n--- Conclusión de la Evaluación de Supuestos ---\")\n",
        "print(\"La evaluación de estos supuestos es crucial para confiar en las inferencias del modelo de regresión lineal.\")\n",
        "print(\"Si los supuestos no se cumplen, las estimaciones de los coeficientes y sus errores estándar pueden no ser válidas,\")\n",
        "print(\"afectando la significancia estadística y la capacidad de generalización del modelo.\")\n",
        "print(\"En caso de incumplimiento, se pueden considerar transformaciones de datos, la inclusión de otras variables,\")\n",
        "print(\"o el uso de modelos de regresión más avanzados (ej. regresión robusta, modelos generalizados lineales).\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(\"/content/sample_data/Data_Regresion.xlsx\", sheet_name='data')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "xbFlGexBgC_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mínimos Cuadrados Generalizados Factibles con GLSAR"
      ],
      "metadata": {
        "id": "GsH3I837kzkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import probplot, shapiro, jarque_bera, normaltest, anderson, kstest\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan, het_white,lilliefors\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.regression.linear_model import GLS, GLSAR\n",
        "\n",
        "\n",
        "\n",
        "# Transformar 'Puntaje(y)' (as per your original code)\n",
        "#data['Puntaje(y)'] = data['Puntaje(y)']**3\n",
        "\n",
        "# --- 1. Definir variables dependiente (Y) e independiente (X) ---\n",
        "X = data[['Horas(x)']] # X debe ser un DataFrame para sm.add_constant\n",
        "y = data['Puntaje(y)']\n",
        "\n",
        "# Añadir una constante a las características para el modelo statsmodels\n",
        "X_const = sm.add_constant(X)\n",
        "\n",
        "print(\"Datos Cargados:\")\n",
        "print(data)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- 2. Construcción del Modelo de Regresión Lineal (OLS inicial para diagnóstico) ---\n",
        "# Usamos statsmodels para tener un resumen detallado y acceso a los residuos para los supuestos.\n",
        "\n",
        "model_ols = sm.OLS(y, X_const)\n",
        "results_ols = model_ols.fit()\n",
        "print(\"Resumen del Modelo de Regresión Lineal (OLS Inicial):\")\n",
        "print(results_ols.summary())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Obtener los residuos del modelo OLS\n",
        "residuals_ols = results_ols.resid\n",
        "fitted_values_ols = results_ols.fittedvalues\n",
        "\n",
        "# --- 3. Evaluación de Supuestos de la Regresión Lineal (con OLS) ---\n",
        "\n",
        "## Supuesto 1: Linealidad 📈\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.scatterplot(x=X['Horas(x)'], y=y)\n",
        "plt.title('Linealidad: X vs. Y (OLS)')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.scatterplot(x=fitted_values_ols, y=residuals_ols)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuos vs. Valores Predichos (OLS)')\n",
        "plt.xlabel('Valores Predichos')\n",
        "plt.ylabel('Residuos')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Evaluación del Supuesto de Linealidad (OLS) ---\")\n",
        "print(\"Observar el gráfico 'X vs. Y': si los puntos siguen una tendencia lineal, el supuesto se cumple.\")\n",
        "print(\"En el gráfico de 'Residuos vs. Valores Predichos', los residuos deben estar distribuidos aleatoriamente alrededor de cero, sin patrones evidentes (forma de embudo, curva, etc.).\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "## Supuesto 2: Normalidad de los Residuos 📊\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(residuals_ols, kde=True)\n",
        "plt.title('Histograma de los Residuos (OLS)')\n",
        "plt.xlabel('Residuos')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "probplot(residuals_ols, dist=\"norm\", plot=plt)\n",
        "plt.title('Q-Q Plot de los Residuos (OLS)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Evaluación del Supuesto de Normalidad de los Residuos (OLS) ---\")\n",
        "print(\"Pruebas estadísticas:\")\n",
        "\n",
        "shapiro_test = shapiro(residuals_ols)\n",
        "print(f\"Shapiro-Wilk Test: Estadístico={shapiro_test.statistic:.3f}, p-value={shapiro_test.pvalue:.3f}\")\n",
        "if shapiro_test.pvalue > 0.05:\n",
        "    print(\"   -> **No se puede rechazar la hipótesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"   -> **Se rechaza la hipótesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "jarque_bera_test = jarque_bera(residuals_ols)\n",
        "print(f\"Jarque-Bera Test: Estadístico={jarque_bera_test.statistic:.3f}, p-value={jarque_bera_test.pvalue:.3f}\")\n",
        "if jarque_bera_test.pvalue > 0.05:\n",
        "    print(\"   -> **No se puede rechazar la hipótesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"   -> **Se rechaza la hipótesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "agostino_test = normaltest(residuals_ols)\n",
        "print(f\"\\nAgostino-Pearson Test: Estadístico={agostino_test.statistic:.3f}, p-value={agostino_test.pvalue:.3f}\")\n",
        "if agostino_test.pvalue > 0.05:\n",
        "    print(\"   -> **No se puede rechazar la hipótesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"   -> **Se rechaza la hipótesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "anderson_test = anderson(residuals_ols, dist='norm')\n",
        "print(f\"\\nAnderson-Darling Test: Estadístico={anderson_test.statistic:.3f}\")\n",
        "print(\"Valores críticos y niveles de significancia:\")\n",
        "for i in range(len(anderson_test.critical_values)):\n",
        "    sl, cv = anderson_test.significance_level[i], anderson_test.critical_values[i]\n",
        "    if anderson_test.statistic < cv:\n",
        "        print(f\"   -> Para nivel de significancia {sl}%: estadístico < {cv:.3f} (No rechazar normalidad)\")\n",
        "    else:\n",
        "        print(f\"   -> Para nivel de significancia {sl}%: estadístico >= {cv:.3f} (Rechazar normalidad)\")\n",
        "\n",
        "ks_test = kstest(residuals_ols, 'norm', args=(residuals_ols.mean(), residuals_ols.std()))\n",
        "print(f\"\\nKolmogorov-Smirnov Test: Estadístico={ks_test.statistic:.3f}, p-value={ks_test.pvalue:.3f}\")\n",
        "if ks_test.pvalue > 0.05:\n",
        "    print(\"   -> **No se puede rechazar la hipótesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"   -> **Se rechaza la hipótesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "lillie_test = lilliefors(residuals_ols, dist='norm')\n",
        "print(f\"\\nLilliefors Test: Estadístico={lillie_test[0]:.3f}, p-value={lillie_test[1]:.3f}\")\n",
        "if lillie_test[1] > 0.05:\n",
        "    print(\"   -> **No se puede rechazar la hipótesis nula**: los residuos parecen ser normales (p > 0.05).\")\n",
        "else:\n",
        "    print(\"   -> **Se rechaza la hipótesis nula**: los residuos no parecen ser normales (p <= 0.05).\")\n",
        "\n",
        "print(\"\\nObservar el Q-Q Plot: los puntos deben seguir aproximadamente la línea recta. Las desviaciones indican no normalidad.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "## Supuesto 3: Homocedasticidad (Varianza Constante de los Residuos) 📏\n",
        "print(\"\\n--- Evaluación del Supuesto de Homocedasticidad (OLS) ---\")\n",
        "print(\"Pruebas estadísticas:\")\n",
        "\n",
        "bp_test = het_breuschpagan(residuals_ols, X_const)\n",
        "print(f\"Breusch-Pagan Test: LM Statistic={bp_test[0]:.3f}, p-value={bp_test[1]:.3f}, F-statistic={bp_test[2]:.3f}, F-p-value={bp_test[3]:.3f}\")\n",
        "if bp_test[1] > 0.05:\n",
        "    print(\"   -> **No se puede rechazar la hipótesis nula**: hay homocedasticidad (p > 0.05).\")\n",
        "else:\n",
        "    print(\"   -> **Se rechaza la hipótesis nula**: hay heterocedasticidad (p <= 0.05).\")\n",
        "\n",
        "white_test = het_white(residuals_ols, X_const)\n",
        "print(f\"White Test: LM Statistic={white_test[0]:.3f}, p-value={white_test[1]:.3f}, F-statistic={white_test[2]:.3f}, F-p-value={white_test[3]:.3f}\")\n",
        "if white_test[1] > 0.05:\n",
        "    print(\"   -> **No se puede rechazar la hipótesis nula**: hay homocedasticidad (p > 0.05).\")\n",
        "else:\n",
        "    print(\"   -> **Se rechaza la hipótesis nula**: hay heterocedasticidad (p <= 0.05).\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "## Supuesto 4: Ausencia de Multicolinealidad 🔄\n",
        "print(\"\\n--- Evaluación del Supuesto de Ausencia de Multicolinealidad ---\")\n",
        "if X.ndim > 1 and X.shape[1] > 1: # Solo se aplica si hay más de una variable predictora\n",
        "    print(\"Matriz de Correlación entre Variables Predictoras:\")\n",
        "    correlation_matrix = X.corr()\n",
        "    print(correlation_matrix)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title('Matriz de Correlación de Variables Predictoras')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nFactor de Inflación de la Varianza (VIF):\")\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"feature\"] = X.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "    print(vif_data)\n",
        "\n",
        "    print(\"\\nInterpretación del VIF:\")\n",
        "    print(\"   - VIF = 1: No hay correlación.\")\n",
        "    print(\"   - VIF entre 1 y 5: Moderada correlación.\")\n",
        "    print(\"   - VIF > 5 (o > 10, según la literatura): Alta multicolinealidad, lo que puede ser problemático.\")\n",
        "else:\n",
        "    print(\"Solo hay una variable predictora (X), por lo que la multicolinealidad no es un problema en este modelo.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "## Supuesto 5: Independencia de los Residuos (Ausencia de Autocorrelación) 🔗\n",
        "print(\"\\n--- Evaluación del Supuesto de Independencia de los Residuos (OLS) ---\")\n",
        "summary_string_ols = str(results_ols.summary())\n",
        "durbin_watson_line_ols = [line for line in summary_string_ols.split('\\n') if 'Durbin-Watson' in line][0]\n",
        "durbin_watson_stat_ols = float(durbin_watson_line_ols.split()[-1])\n",
        "\n",
        "print(f\"Estadístico Durbin-Watson (del resumen del modelo OLS): {durbin_watson_stat_ols:.3f}\")\n",
        "print(\"Interpretación del Durbin-Watson:\")\n",
        "print(\"   - Valor **cercano a 2**: No hay autocorrelación.\")\n",
        "print(\"   - Valor **< 2**: Autocorrelación positiva (residuos adyacentes similares).\")\n",
        "print(\"   - Valor **> 2**: Autocorrelación negativa (residuos adyacentes opuestos).\")\n",
        "print(\"   - Un valor entre 1.5 y 2.5 generalmente se considera aceptable.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- Implementación de FGLS para Autocorrelación (usando GLSAR) ---\n",
        "# GLSAR es una implementación de GLS para corregir la autocorrelación.\n",
        "# Estima el parámetro de autocorrelación (rho) y luego realiza la transformación.\n",
        "\n",
        "print(\"\\n--- Implementación de FGLS para corregir Autocorrelación (usando GLSAR) ---\")\n",
        "\n",
        "# Para GLSAR, necesitamos un modelo GLSAR y el número de lags para el AR(p) proceso.\n",
        "# Generalmente, se empieza con AR(1) (lags=1) si el Durbin-Watson indica autocorrelación positiva.\n",
        "# Si el Durbin-Watson es bajo (mucho menor que 2), sugiere autocorrelación positiva de primer orden.\n",
        "\n",
        "# Ajustar un modelo GLSAR con un proceso AR(1)\n",
        "model_glsar = GLSAR(y, X_const, 1) # 1 indica AR(1)\n",
        "results_glsar = model_glsar.iterative_fit()\n",
        "\n",
        "print(\"\\nResumen del Modelo de Regresión con FGLS (GLSAR - AR(1)):\")\n",
        "print(results_glsar.summary())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Obtener los residuos y valores predichos del modelo GLSAR\n",
        "residuals_glsar = results_glsar.resid\n",
        "fitted_values_glsar = results_glsar.fittedvalues\n",
        "\n",
        "# --- Re-evaluación del Supuesto de Independencia de los Residuos (con FGLS) ---\n",
        "print(\"\\n--- Re-evaluación del Supuesto de Independencia de los Residuos (después de FGLS) ---\")\n",
        "summary_string_glsar = str(results_glsar.summary())\n",
        "durbin_watson_line_glsar = [line for line in summary_string_glsar.split('\\n') if 'Durbin-Watson' in line][0]\n",
        "durbin_watson_stat_glsar = float(durbin_watson_line_glsar.split()[-1])\n",
        "\n",
        "print(f\"Estadístico Durbin-Watson (del resumen del modelo GLSAR): {durbin_watson_stat_glsar:.3f}\")\n",
        "print(\"Interpretación del Durbin-Watson:\")\n",
        "print(\"   - Valor **cercano a 2**: No hay autocorrelación.\")\n",
        "print(\"   - Valor **< 2**: Autocorrelación positiva (residuos adyacentes similares).\")\n",
        "print(\"   - Valor **> 2**: Autocorrelación negativa (residuos adyacentes opuestos).\")\n",
        "print(\"   - Un valor entre 1.5 y 2.5 generalmente se considera aceptable.\")\n",
        "print(\"\\nDespués de aplicar FGLS, esperamos que el estadístico Durbin-Watson se acerque a 2.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- Visualización de Residuos después de FGLS ---\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.scatterplot(x=fitted_values_glsar, y=residuals_glsar)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuos vs. Valores Predichos (GLSAR)')\n",
        "plt.xlabel('Valores Predichos (GLSAR)')\n",
        "plt.ylabel('Residuos (GLSAR)')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(residuals_glsar, kde=True)\n",
        "plt.title('Histograma de Residuos (GLSAR)')\n",
        "plt.xlabel('Residuos (GLSAR)')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Conclusión de la Evaluación de Supuestos ---\")\n",
        "print(\"La evaluación de estos supuestos es crucial para confiar en las inferencias del modelo de regresión lineal.\")\n",
        "print(\"Si los supuestos no se cumplen, las estimaciones de los coeficientes y sus errores estándar pueden no ser válidas,\")\n",
        "print(\"afectando la significancia estadística y la capacidad de generalización del modelo.\")\n",
        "print(\"En caso de incumplimiento, se pueden considerar transformaciones de datos, la inclusión de otras variables,\")\n",
        "print(\"o el uso de modelos de regresión más avanzados (ej. regresión robusta, modelos generalizados lineales).\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mgNnPb5h4DG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OFIGabzTgbSk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aqcb1OCVfRVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1a4d6fc"
      },
      "source": [
        "##Mínimos cuadrados ponderados (wls)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5abdd950"
      },
      "source": [
        "# --- Implementación de Mínimos Cuadrados Ponderados (WLS) ---\n",
        "\n",
        "print(\"\\n--- Implementación de Mínimos Cuadrados Ponderados (WLS) ---\")\n",
        "\n",
        "# 1. Explicación de WLS\n",
        "print(\"\\n**Concepto de Mínimos Cuadrados Ponderados (WLS)**\")\n",
        "print(\"Mínimos Cuadrados Ponderados (WLS) es un método de regresión que se utiliza\")\n",
        "print(\"cuando el supuesto de homocedasticidad de los errores se viola (es decir, hay heterocedasticidad).\")\n",
        "print(\"La heterocedasticidad implica que la varianza de los errores no es constante\")\n",
        "print(\"a lo largo de los diferentes niveles de las variables predictoras.\")\n",
        "print(\"En WLS, se asignan pesos a cada observación. Las observaciones con menor varianza (errores más pequeños)\")\n",
        "print(\"reciben pesos mayores, y las observaciones con mayor varianza (errores más grandes)\")\n",
        "print(\"reciben pesos menores. Esto le da más 'importancia' en el ajuste del modelo a las observaciones\")\n",
        "print(\"que son más fiables (tienen menor varianza), corrigiendo así el problema de la heterocedasticidad\")\n",
        "print(\"y produciendo estimaciones de coeficientes más eficientes (menores errores estándar).\")\n",
        "print(\"Generalmente, los pesos son inversamente proporcionales a la varianza de los errores.\")\n",
        "\n",
        "# 2. Definir los pesos\n",
        "# Una forma común de estimar los pesos es usar los residuos del modelo OLS inicial.\n",
        "# Si la varianza de los errores es proporcional a alguna función de las variables predictoras\n",
        "# o de los valores ajustados, podemos usar esa función para definir los pesos.\n",
        "# Por ejemplo, si la varianza es proporcional a los valores ajustados al cuadrado (fitted_values_ols**2),\n",
        "# los pesos serían inversamente proporcionales a fitted_values_ols**2.\n",
        "# Para evitar dividir por cero o valores muy pequeños, a menudo se añade una pequeña constante.\n",
        "\n",
        "# Estimación de los pesos: Inverso de la varianza estimada de los residuos OLS\n",
        "# Una aproximación simple es usar el inverso de los residuos al cuadrado del modelo OLS.\n",
        "# Sin embargo, esto puede ser problemático si los residuos son cercanos a cero.\n",
        "# Una alternativa común es usar el inverso de los valores ajustados al cuadrado o alguna transformación de X.\n",
        "# Aquí, usaremos el inverso de los valores ajustados del modelo OLS.\n",
        "# Se recomienda añadir una pequeña constante para evitar la división por cero.\n",
        "weights = 1.0 / (fitted_values_ols**2 + 1e-6) # Añadir una pequeña constante para estabilidad\n",
        "\n",
        "print(\"\\nPesos calculados (basados en el inverso de los valores ajustados del modelo OLS):\")\n",
        "print(weights.head())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 3. Ajustar el modelo WLS\n",
        "# Usamos la clase WLS de statsmodels, pasando la variable dependiente, las predictoras con constante, y los pesos.\n",
        "model_wls = sm.WLS(y, X_const, weights=weights)\n",
        "results_wls = model_wls.fit()\n",
        "\n",
        "# 4. Mostrar el resumen de los resultados del modelo WLS\n",
        "print(\"\\nResumen del Modelo de Regresión con WLS:\")\n",
        "print(results_wls.summary())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 5. Explicar cómo los pesos influyen en el ajuste y los resultados\n",
        "print(\"\\n**Influencia de los Pesos en el Modelo WLS**\")\n",
        "print(\"En el modelo WLS, las observaciones con pesos más altos (generalmente aquellas con menor varianza de error estimada)\")\n",
        "print(\"tienen una mayor influencia en la determinación de los coeficientes de regresión.\")\n",
        "print(\"Esto significa que el modelo se ajusta más estrechamente a los puntos de datos que se consideran más 'fiables' (menos ruidosos).\")\n",
        "print(\"La principal ventaja es que las estimaciones de los coeficientes son más eficientes (tienen menores errores estándar)\")\n",
        "print(\"en presencia de heterocedasticidad, en comparación con OLS.\")\n",
        "print(\"Comparando el resumen de WLS con el de OLS, se observará que los errores estándar de los coeficientes son diferentes,\")\n",
        "print(\"y potencialmente menores, lo que puede afectar la significancia estadística (valores p).\")\n",
        "print(\"El R-cuadrado en WLS se interpreta de manera diferente a OLS y a menudo no es directamente comparable.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# Obtener los residuos y valores predichos del modelo WLS\n",
        "residuals_wls = results_wls.resid\n",
        "fitted_values_wls = results_wls.fittedvalues\n",
        "\n",
        "# 6. Incluir visualizaciones (residuos ponderados vs. valores ajustados)\n",
        "# Para evaluar la homocedasticidad después de aplicar WLS, es útil graficar los residuos ponderados\n",
        "# (residuos * sqrt(weights)) o los residuos sin ponderar frente a los valores ajustados.\n",
        "# Idealmente, los residuos ponderados vs. valores ajustados deberían mostrar una dispersión más uniforme alrededor de cero.\n",
        "\n",
        "# Calcular residuos ponderados\n",
        "weighted_residuals_wls = residuals_wls * np.sqrt(weights)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Gráfico de Residuos WLS sin ponderar vs. Valores Predichos WLS\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.scatterplot(x=fitted_values_wls, y=residuals_wls)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuos (WLS) vs. Valores Predichos (WLS)')\n",
        "plt.xlabel('Valores Predichos (WLS)')\n",
        "plt.ylabel('Residuos (WLS)')\n",
        "\n",
        "# Gráfico de Residuos Ponderados WLS vs. Valores Predichos WLS\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.scatterplot(x=fitted_values_wls, y=weighted_residuals_wls)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuos Ponderados (WLS) vs. Valores Predichos (WLS)')\n",
        "plt.xlabel('Valores Predichos (WLS)')\n",
        "plt.ylabel('Residuos Ponderados (WLS)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Evaluación de Homocedasticidad (después de WLS) ---\")\n",
        "print(\"Observar el gráfico de 'Residuos Ponderados (WLS) vs. Valores Predichos (WLS)'.\")\n",
        "print(\"Esperamos que la dispersión de los puntos sea más uniforme alrededor de la línea cero\")\n",
        "print(\"en comparación con el gráfico de residuos vs. valores predichos del modelo OLS inicial.\")\n",
        "print(\"Esto indicaría que WLS ha ayudado a corregir la heterocedasticidad.\")\n",
        "print(\"-\" * 50)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2bfee15"
      },
      "source": [
        "## Regresión Lineal Robusta\n",
        ".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "059068ac"
      },
      "source": [
        "# --- Implementación de Regresión Lineal Robusta ---\n",
        "\n",
        "print(\"\\n--- Implementación de Regresión Lineal Robusta ---\")\n",
        "\n",
        "# 1. Explicación de Regresión Lineal Robusta\n",
        "print(\"\\n**Concepto de Regresión Lineal Robusta**\")\n",
        "print(\"La Regresión Lineal Robusta es una alternativa a Mínimos Cuadrados Ordinarios (OLS)\")\n",
        "print(\"que es menos sensible a la presencia de valores atípicos (outliers) o influyentes\")\n",
        "print(\"en los datos. Mientras que OLS minimiza la suma de los errores al cuadrado,\")\n",
        "print(\"lo que hace que los valores atípicos tengan un impacto cuadrático grande en el ajuste,\")\n",
        "print(\"la regresión robusta utiliza diferentes funciones de pérdida o esquemas de ponderación\")\n",
        "print(\"para mitigar la influencia de estas observaciones extremas.\")\n",
        "print(\"Se utiliza típicamente cuando la inspección visual de los datos o el análisis de residuos\")\n",
        "print(\"del modelo OLS inicial sugiere la presencia de outliers que podrían distorsionar los resultados de OLS.\")\n",
        "\n",
        "# 2. Métodos de Regresión Robusta en statsmodels\n",
        "print(\"\\n**Métodos de Regresión Robusta en `statsmodels`**\")\n",
        "print(\"`statsmodels` implementa varios métodos de estimación robusta.\")\n",
        "print(\"Los más comunes son los M-estimators (Estimadores M).\")\n",
        "print(\"Los M-estimators generalizan la idea de OLS minimizando una función de pérdida (rho)\")\n",
        "print(\"de los residuos, en lugar del cuadrado de los residuos. Equivalente a esto,\")\n",
        "print(\"ponderan las observaciones de manera iterativa en función del tamaño de sus residuos;\")\n",
        "print(\"los residuos más grandes reciben un peso menor.\")\n",
        "print(\"Algunas funciones de ponderación (normas) comunes para M-estimators incluyen:\")\n",
        "print(\"  - Huber's T (HuberT): Pondera linealmente los residuos grandes.\")\n",
        "print(\"  - Tukey's Biweight (TukeyBiweight): Asigna peso cero a residuos que exceden un cierto umbral,\")\n",
        "print(\"    eliminando efectivamente su influencia.\")\n",
        "print(\"La clase `statsmodels.api.RLM` (Robust Linear Model) implementa M-estimators.\")\n",
        "print(\"Por defecto, `RLM` utiliza la norma de Huber (HuberT).\")\n",
        "\n",
        "# 3. Importar la clase RLM (ya importada en una celda anterior)\n",
        "# from statsmodels.api import RLM # No es necesario importar de nuevo\n",
        "\n",
        "# 4. Instanciar un modelo de regresión robusta\n",
        "# Usamos la clase RLM de statsmodels. Podemos especificar una función de ponderación (norm).\n",
        "# Por defecto, usa HuberT. Aquí usaremos HuberT explícitamente o dejaremos el valor por defecto.\n",
        "# También podemos usar TukeyBiweight: sm.robust.norms.TukeyBiweight()\n",
        "\n",
        "model_robust = sm.RLM(y, X_const, M=sm.robust.norms.HuberT())\n",
        "# O simplemente: model_robust = sm.RLM(y, X_const) # Usa HuberT por defecto\n",
        "\n",
        "# 5. Ajustar el modelo robusto\n",
        "results_robust = model_robust.fit()\n",
        "\n",
        "# 6. Mostrar el resumen de los resultados del modelo de regresión robusta\n",
        "print(\"\\nResumen del Modelo de Regresión Robusta (usando HuberT):\")\n",
        "print(results_robust.summary())\n",
        "print(\"-\" * 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1f466cf"
      },
      "source": [
        "# 7. Obtener los residuos y valores predichos del modelo robusto\n",
        "residuals_robust = results_robust.resid\n",
        "fitted_values_robust = results_robust.fittedvalues\n",
        "\n",
        "print(\"\\nResiduos del modelo Robusto (HuberT):\")\n",
        "print(residuals_robust.head())\n",
        "print(\"\\nValores predichos del modelo Robusto (HuberT):\")\n",
        "print(fitted_values_robust.head())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 8. Visualizar los residuos del modelo robusto\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Gráfico de Residuos Robusto vs. Valores Predichos Robusto\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.scatterplot(x=fitted_values_robust, y=residuals_robust)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuos (Robusto - HuberT) vs. Valores Predichos (Robusto)')\n",
        "plt.xlabel('Valores Predichos (Robusto)')\n",
        "plt.ylabel('Residuos (Robusto)')\n",
        "\n",
        "# Gráfico de Residuos OLS vs. Valores Predichos OLS para comparación (usando variables de celdas anteriores)\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.scatterplot(x=fitted_values_ols, y=residuals_ols)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title('Residuos (OLS) vs. Valores Predichos (OLS) - Comparación')\n",
        "plt.xlabel('Valores Predichos (OLS)')\n",
        "plt.ylabel('Residuos (OLS)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Evaluación Visual de Residuos (Robusto vs. OLS) ---\")\n",
        "print(\"Observar el gráfico de 'Residuos (Robusto - HuberT) vs. Valores Predichos (Robusto)'.\")\n",
        "print(\"La dispersión de los puntos puede ser diferente a la del gráfico OLS,\")\n",
        "print(\"especialmente si hay valores atípicos. Los puntos atípicos que tenían\")\n",
        "print(\"una gran desviación en OLS pueden tener una menor influencia o un patrón diferente\")\n",
        "print(\"en el modelo robusto debido a la reponderación.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# 9. Imprimir una breve interpretación de los resultados del modelo robusto\n",
        "print(\"\\n--- Interpretación de los Resultados del Modelo Robusto (HuberT) ---\")\n",
        "print(\"Comparando el resumen del Modelo Robusto con el del Modelo OLS inicial:\")\n",
        "print(f\"- Coeficiente 'const' (Robusto): {results_robust.params['const']:.3f} (OLS: {results_ols.params['const']:.3f})\")\n",
        "print(f\"- Coeficiente 'Horas(x)' (Robusto): {results_robust.params['Horas(x)']:.3f} (OLS: {results_ols.params['Horas(x)']:.3f})\")\n",
        "print(f\"- Error Estándar 'const' (Robusto): {results_robust.bse['const']:.3f} (OLS: {results_ols.bse['const']:.3f})\")\n",
        "print(f\"- Error Estándar 'Horas(x)' (Robusto): {results_robust.bse['Horas(x)']:.3f} (OLS: {results_ols.bse['Horas(x)']:.3f})\")\n",
        "\n",
        "print(\"\\nObservaciones:\")\n",
        "print(\"Los coeficientes estimados por el modelo robusto pueden diferir de los de OLS,\")\n",
        "print(\"especialmente si hay valores atípicos que influyen en el ajuste de OLS.\")\n",
        "print(\"Los errores estándar del modelo robusto a menudo se calculan de manera diferente\")\n",
        "print(\"(por defecto, usa Cov Type H1 en statsmodels RLM, que es robusta a la heterocedasticidad)\")\n",
        "print(\"y pueden proporcionar inferencias más fiables en presencia de desviaciones de los supuestos de OLS.\")\n",
        "print(\"La regresión robusta, al ponderar menos los outliers, tiende a ajustar la 'tendencia central' de los datos,\")\n",
        "print(\"siendo menos arrastrada por puntos extremos.\")\n",
        "print(\"En este caso, el coeficiente de 'Horas(x)' es ligeramente diferente y el intercepto también,\")\n",
        "print(\"lo que sugiere que el modelo robusto ha ajustado el ajuste para ser menos influenciado por\")\n",
        "print(\"las observaciones con residuos grandes (los posibles outliers identificados en el análisis OLS anterior).\")\n",
        "print(\"-\" * 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "862127ae"
      },
      "source": [
        "## Modelos Lineales Generalizados\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e03f43b3"
      },
      "source": [
        "# --- Implementación de Modelos Lineales Generalizados (GLM) ---\n",
        "\n",
        "print(\"\\n--- Implementación de Modelos Lineales Generalizados (GLM) ---\")\n",
        "\n",
        "# 1. Explicación de GLM\n",
        "print(\"\\n**Concepto de Modelos Lineales Generalizados (GLM)**\")\n",
        "print(\"Los Modelos Lineales Generalizados (GLM) extienden el modelo de regresión lineal clásico (OLS)\")\n",
        "print(\"para permitir que la variable dependiente (respuesta) tenga una distribución de error\")\n",
        "print(\"distinta a la normal y para relacionar la media de la respuesta con una combinación\")\n",
        "print(\"lineal de las variables predictoras a través de una **función de enlace**.\")\n",
        "print(\"El modelo lineal clásico asume que:\")\n",
        "print(\"  - La variable dependiente es continua.\")\n",
        "print(\"  - Los errores son independientes y se distribuyen normalmente con media cero y varianza constante (homocedasticidad).\")\n",
        "print(\"  - La relación entre la media de la variable dependiente y las predictoras es lineal.\")\n",
        "print(\"\\nGLM relaja estas suposiciones al especificar:\")\n",
        "print(\"  - Una **distribución de probabilidad** para la variable dependiente (ej. Binomial para datos binarios, Poisson para conteos, Gamma para datos positivos asimétricos).\")\n",
        "print(\"  - Una **función de enlace** que transforma la media esperada de la variable dependiente para que la relación con la combinación lineal de las predictoras sea lineal. G(E[y]) = Xβ.\")\n",
        "print(\"  - Una **función de varianza** que describe cómo la varianza de la respuesta depende de la media.\")\n",
        "\n",
        "# 2. Ejemplos comunes de GLM\n",
        "print(\"\\n**Ejemplos Comunes de GLM:**\")\n",
        "print(\"  - **Regresión Logística:** Para variables dependientes binarias (0/1). Utiliza la familia Binomial y la función de enlace logit.\")\n",
        "print(\"  - **Regresión de Poisson:** Para variables dependientes de conteo (números enteros no negativos). Utiliza la familia Poisson y la función de enlace log.\")\n",
        "print(\"  - **Regresión Gamma:** Para variables dependientes continuas y positivas con distribuciones asimétricas (ej. tiempos de espera, reclamaciones de seguros). Utiliza la familia Gamma y una función de enlace como la inversa o la log.\")\n",
        "print(\"  - **Regresión Lineal (como un caso especial de GLM):** Utiliza la familia Gaussiana (Normal) y la función de enlace identidad.\")\n",
        "\n",
        "\n",
        "# 3. Preparar datos para un ejemplo básico de GLM (Regresión de Poisson)\n",
        "# El conjunto de datos actual ('data') es continuo y no es ideal para Poisson o Logística.\n",
        "# Simularemos datos simples para una Regresión de Poisson.\n",
        "# Supongamos que queremos modelar el número de eventos (un conteo) en función de una variable predictora.\n",
        "\n",
        "np.random.seed(42) # para reproducibilidad\n",
        "n_samples = 100\n",
        "# Variable predictora (ej. nivel de exposición, tiempo)\n",
        "X_sim = np.random.rand(n_samples) * 10\n",
        "# Combinación lineal de predictoras (en este caso, solo una)\n",
        "linear_predictor = 0.5 + 0.3 * X_sim\n",
        "# Media esperada para una distribución de Poisson (lambda = exp(linear_predictor))\n",
        "# La función de enlace log (log(lambda) = linear_predictor) es la canónica para Poisson.\n",
        "mu_sim = np.exp(linear_predictor)\n",
        "# Generar datos de conteo siguiendo una distribución de Poisson con la media calculada\n",
        "y_sim = np.random.poisson(mu_sim)\n",
        "\n",
        "# Crear un DataFrame para los datos simulados\n",
        "data_glm = pd.DataFrame({'X_sim': X_sim, 'y_sim': y_sim})\n",
        "\n",
        "# Añadir una constante a las características para el modelo statsmodels\n",
        "X_sim_const = sm.add_constant(data_glm['X_sim'])\n",
        "y_sim = data_glm['y_sim']\n",
        "\n",
        "print(\"\\nDatos Simulados para Regresión de Poisson:\")\n",
        "print(data_glm.head())\n",
        "print(\"-\" * 50)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19143f6d"
      },
      "source": [
        "# 4. Instanciar un modelo GLM (Regresión de Poisson)\n",
        "# Usamos la clase GLM de statsmodels, especificando la variable dependiente, las predictoras con constante,\n",
        "# y la familia de distribución de errores (Poisson en este caso).\n",
        "\n",
        "model_glm = sm.GLM(y_sim, X_sim_const, family=sm.families.Poisson())\n",
        "\n",
        "# 5. Ajustar el modelo GLM\n",
        "results_glm = model_glm.fit()\n",
        "\n",
        "# 6. Mostrar el resumen de los resultados del modelo GLM\n",
        "print(\"\\nResumen del Modelo GLM (Regresión de Poisson):\")\n",
        "print(results_glm.summary())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 7. Explicar las diferencias clave en la interpretación de los resultados de un GLM\n",
        "print(\"\\n**Diferencias Clave en la Interpretación de Resultados (GLM vs. OLS)**\")\n",
        "print(\"Comparando el resumen del Modelo GLM (Poisson) con el del Modelo OLS:\")\n",
        "print(\"\\n**Coeficientes:**\")\n",
        "print(\"  - En OLS, los coeficientes se interpretan directamente como el cambio esperado en la media de Y por un cambio de una unidad en X, manteniendo otras variables constantes.\")\n",
        "print(\"  - En GLM, los coeficientes se interpretan en la **escala de la función de enlace**. Para la Regresión de Poisson con enlace log, el coeficiente de una predictora\")\n",
        "print(\"    representa el cambio esperado en el **logaritmo de la media** de Y por un cambio de una unidad en X.\")\n",
        "print(\"    Esto significa que un cambio de una unidad en X multiplica la media esperada de Y por exp(coeficiente).\")\n",
        "print(\"    Por ejemplo, si el coeficiente es 0.3, un aumento de 1 en X multiplica la media esperada por exp(0.3) ≈ 1.35 (un aumento del 35%).\")\n",
        "\n",
        "print(\"\\n**Bondad de Ajuste:**\")\n",
        "print(\"  - OLS utiliza R-cuadrado como métrica principal. Mide la proporción de la varianza en Y explicada por el modelo.\")\n",
        "print(\"  - GLM no usa R-cuadrado de la misma manera. Las métricas de bondad de ajuste para GLM a menudo se basan en la **devianza**, que es una generalización de la suma de cuadrados residuales.\")\n",
        "print(\"    La devianza residual mide la discrepancia entre el modelo ajustado y los datos saturados (un modelo que ajusta perfectamente cada observación).\")\n",
        "print(\"    Una devianza residual pequeña en relación con los grados de libertad sugiere un buen ajuste.\")\n",
        "print(\"    También se puede usar la devianza nula (deviance of the null model, con solo el intercepto) para calcular métricas análogas al R-cuadrado (pseudo R-squared),\")\n",
        "print(\"    como el Pseudo R-squared de McFadden, Cox-Snell, o Nagelkerke, aunque su interpretación es diferente a la de OLS.\")\n",
        "print(\"    El AIC (Criterio de Información de Akaike) y BIC (Criterio de Información Bayesiano) también se utilizan para comparar modelos GLM.\")\n",
        "\n",
        "print(\"\\n**Supuestos:**\")\n",
        "print(\"  - OLS asume normalidad, homocedasticidad e independencia de los errores.\")\n",
        "print(\"  - GLM asume que la variable dependiente sigue la distribución de la familia especificada y que los errores son independientes.\")\n",
        "print(\"    No asume homocedasticidad (la varianza puede depender de la media, según la familia) ni normalidad de los errores en la escala original.\")\n",
        "\n",
        "print(\"\\n**Errores Estándar y Significancia:**\")\n",
        "print(\"  - Los errores estándar y los valores p se interpretan de manera similar (para probar la significancia de los coeficientes),\")\n",
        "print(\"    pero se derivan de la teoría de máxima verosimilitud, que es la base de la estimación de GLM, en lugar de OLS.\")\n",
        "\n",
        "# 8. Concluir resaltando cuándo usar GLM\n",
        "print(\"\\n--- Cuándo Considerar el Uso de un GLM ---\")\n",
        "print(\"Debe considerar usar un GLM en lugar de OLS cuando:\")\n",
        "print(\"  - Su variable dependiente **no es continua** o **no tiene errores distribuidos normalmente**.\")\n",
        "print(\"    (ej. binaria, conteo, proporción, datos positivos asimétricos).\")\n",
        "print(\"  - La **varianza de los errores no es constante** y/o depende de la media de la respuesta.\")\n",
        "print(\"  - La relación entre la media de la respuesta y las predictoras **no es lineal** en la escala original,\")\n",
        "print(\"    pero puede ser lineal en la escala transformada por una función de enlace adecuada.\")\n",
        "print(\"GLM proporciona un marco flexible para modelar una variedad más amplia de tipos de datos de respuesta que OLS.\")\n",
        "print(\"-\" * 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c21148b"
      },
      "source": [
        "La Regresión por Mínimos Cuadrados Ponderados (WLS) aborda la heterocedasticidad asignando pesos a las observaciones, dando más importancia a aquellas con menor varianza de error. Un enfoque común es usar pesos inversamente proporcionales a los valores ajustados al cuadrado de un modelo OLS inicial. El resumen del modelo WLS muestra errores estándar ajustados en comparación con OLS, y los gráficos de residuos ponderados pueden indicar si la heterocedasticidad se ha mitigado.\n",
        "\n",
        "Los Mínimos Cuadrados Generalizados (GLS) son un método más general que OLS y WLS, capaz de manejar tanto la heterocedasticidad como la autocorrelación. Su implementación en statsmodels requiere especificar la matriz de covarianza de error (Ω). Si Ω es diagonal con entradas inversamente proporcionales a los pesos de WLS, los resultados de GLS serán numéricamente idénticos a WLS. En la práctica, Ω es a menudo desconocida y necesita ser estimada (GLS Factible - FGLS).\n",
        "\n",
        "La Regresión Lineal Robusta es menos sensible a los valores atípicos que OLS. Los estimadores M, como Huber's T o Tukey's Biweight, son métodos comunes que re-ponderan iterativamente las observaciones basándose en sus residuos, reduciendo la influencia de los residuos grandes. El resumen del modelo robusto y los gráficos de residuos pueden mostrar diferencias en las estimaciones de los coeficientes y los errores estándar de los errores en comparación con OLS, lo que refleja el impacto reducido de los valores atípicos.\n",
        "\n",
        "Los Modelos Lineales Generalizados (GLM) extienden la regresión lineal a variables dependientes que no son continuas o normalmente distribuidas (por ejemplo, binarias, de recuento). Los GLM requieren especificar una distribución de probabilidad para la respuesta (por ejemplo, Binomial, Poisson) y una función de enlace para relacionar la media de la respuesta con el predictor lineal. La interpretación de los coeficientes se realiza en la escala de la función de enlace, no en la escala de respuesta original, y la bondad del ajuste se evalúa utilizando métricas como la deviance y el pseudo R-cuadrado en lugar del R-cuadrado de OLS.\n",
        "\n"
      ]
    }
  ]
}